{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b7476a",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4 Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89d459",
   "metadata": {},
   "source": [
    "This homework is an extension of homework 4, where you will be implementing the Transformer architecture. For this assignment, all the things you need to implement is in the file `python/needle/nn/nn_transformer.py`. Other things in the needle library remains the same. This homework extension is built on homework 4, so make sure to copy the solutions from homework 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9fb467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/pybind11/include (found version \"2.13.1\")\n",
      "-- Found cuda, building cuda backend\n",
      "Sun Aug 11 22:18:38 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090         On | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 50%   38C    P8               33W / 350W|     23MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090         On | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 30%   38C    P8               30W / 350W|      3MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.6 8.6\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /root/workspace/LightTorch/archive/hw4_extra/build\n",
      "make[1]: Entering directory '/root/workspace/LightTorch/archive/hw4_extra/build'\n",
      "make[2]: Entering directory '/root/workspace/LightTorch/archive/hw4_extra/build'\n",
      "make[3]: Entering directory '/root/workspace/LightTorch/archive/hw4_extra/build'\n",
      "make[3]: Leaving directory '/root/workspace/LightTorch/archive/hw4_extra/build'\n",
      "[ 50%] Built target ndarray_backend_cuda\n",
      "make[3]: Entering directory '/root/workspace/LightTorch/archive/hw4_extra/build'\n",
      "make[3]: Leaving directory '/root/workspace/LightTorch/archive/hw4_extra/build'\n",
      "[100%] Built target ndarray_backend_cpu\n",
      "make[2]: Leaving directory '/root/workspace/LightTorch/archive/hw4_extra/build'\n",
      "make[1]: Leaving directory '/root/workspace/LightTorch/archive/hw4_extra/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45349235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54d7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5945207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PTB dataset\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea5c0a",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2f639",
   "metadata": {},
   "source": [
    "In the previous homework you have implemented two sequence models, the Recurrent Neural Network, and Long Short-Term Memory. These models were once the state-of-the-art and default architecture choices on sequence modelling tasks, including language generation, until recently when the famous paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani et al. 2017) came out in 2017. Since then, Transformers, a model architecture introduced in the aforementioned paper, have become the standard and most performant class of model on language tasks. \n",
    "\n",
    "You will be implementing a Transformer in `python/needle/nn/nn_transformer.py`.\n",
    "\n",
    "Transformers are composed of three mains components that you will implement. \n",
    "1. A masked multi-head attention mechanism that adaptively focuses on different timesteps of a sequence. \n",
    "2. A residual block consisting of the attention layer followed by a two-layer neural network applied independently at each timestep. \n",
    "3. A Transformer model consisting of several stacked residual blocks (in this homework you will implement a decoder-only transformer).\n",
    "\n",
    "![model](https://miro.medium.com/v2/1*ZCFSvkKtppgew3cc7BIaug.png)\n",
    "\n",
    "The above is a photo of the Transformer architecture from Vaswani et al. 2017. The version of the transformer you will implement is nearly identical, but has layer normalization applied at the start of each residual block (referred to as a [prenorm variant](https://arxiv.org/abs/2002.04745) of the Transformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094ff30",
   "metadata": {},
   "source": [
    "## Part 1: Implementing the Multi-Head Attention Activation Layer\n",
    "\n",
    "In this subproblem, you will be implementing the `forward` function of a \"base\" attention activation layer `MultiHeadAttention` in `python/needle/nn/nn_transformer.py`. This activation layer will take in three inputs: \n",
    "<p style=\"text-align: center;\">multi-head queries $Q \\in R^\\mathcal{B \\times H \\times T \\times D}$, keys $K \\in R^\\mathcal{B \\times H \\times T \\times D}$, and values $V \\in R^\\mathcal{B \\times H \\times T \\times D}$</p>\n",
    "\n",
    "where $B$ is the batch size, $H$ is the number of attention heads, $T$ is the sequence length, and $D$ is the hidden dimension. \n",
    "\n",
    "The attention output $X \\in R^{B \\times H \\times T \\times D}$ is computed as follows:\n",
    "\n",
    "<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q K^T}{\\sqrt{D}}) V$</p>\n",
    "\n",
    "Note that the matrix multiplications above are batched. This functionality is not natively supported in needle yet, so we have provided a convenient function `matmul` for batched matrix multiplications in `MultiHeadAttention`. Your goal in this section is to return $X$ given the input queries, keys, and values. \n",
    "\n",
    "For auto-regressive Transformer, this attention should support causal masking using the function `self.create_causal_mask` we have provided. This is to make sure that the prediction of next token only depends on it's previous tokens. Specifically, causal masking is applying a mask before the softmax so that the softmax probability is computed over a masked matrix of $\\frac{Q K^T}{\\sqrt{D}}$. \n",
    "\n",
    "In addition, your implementation should apply dropout to the attention softmax $\\text{softmax}(\\frac{Q K^T}{\\sqrt{D}})$. You can use the `self.dropout` function of the `MultiHeadAttention` module.\n",
    "\n",
    "Importantly, this layer is only an activation function, and has no trainable variables (these come later).\n",
    "\n",
    "Once you have finished your implementation, test your code with the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7eeaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.19, pytest-8.3.1, pluggy-1.5.0 -- /root/miniconda3/envs/LightTorch/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /root/workspace/LightTorch/archive/hw4_extra\n",
      "plugins: anyio-4.4.0\n",
      "collected 112 items / 96 deselected / 16 selected                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m16 passed\u001b[0m, \u001b[33m96 deselected\u001b[0m\u001b[32m in 2.90s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"attention_activation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19da8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.19, pytest-8.3.1, pluggy-1.5.0\n",
      "rootdir: /root/workspace/LightTorch/archive/hw4_extra\n",
      "plugins: anyio-4.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
      "collected 4 items / 3 deselected / 1 selected                                  \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py \u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________ submit_attention_activation __________________________\u001b[0m\n",
      "\n",
      "self = <urllib3.connectionpool.HTTPSConnectionPool object at 0x7f7e3ebe9eb0>\n",
      "conn = <urllib3.connection.HTTPSConnection object at 0x7f7e3ec0b6d0>\n",
      "method = 'POST'\n",
      "url = '/_/api/submission?user_key=YOUR_KEY_HERE&func_name=attention_activation'\n",
      "body = None\n",
      "headers = {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '0'}\n",
      "retries = Retry(total=0, connect=None, read=False, redirect=None, status=None)\n",
      "timeout = Timeout(connect=None, read=None, total=None), chunked = False\n",
      "response_conn = <urllib3.connection.HTTPSConnection object at 0x7f7e3ec0b6d0>\n",
      "preload_content = False, decode_content = False, enforce_content_length = True\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_make_request\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        conn: BaseHTTPConnection,\u001b[90m\u001b[39;49;00m\n",
      "        method: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        url: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        body: _TYPE_BODY | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        headers: typing.Mapping[\u001b[96mstr\u001b[39;49;00m, \u001b[96mstr\u001b[39;49;00m] | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        retries: Retry | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\u001b[90m\u001b[39;49;00m\n",
      "        chunked: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        response_conn: BaseHTTPConnection | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        preload_content: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        decode_content: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        enforce_content_length: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> BaseHTTPResponse:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Perform a request on a given urllib connection object taken from our\u001b[39;49;00m\n",
      "    \u001b[33m    pool.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param conn:\u001b[39;49;00m\n",
      "    \u001b[33m        a connection from one of our connection pools\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param method:\u001b[39;49;00m\n",
      "    \u001b[33m        HTTP request method (such as GET, POST, PUT, etc.)\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param url:\u001b[39;49;00m\n",
      "    \u001b[33m        The URL to perform the request on.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param body:\u001b[39;49;00m\n",
      "    \u001b[33m        Data to send in the request body, either :class:`str`, :class:`bytes`,\u001b[39;49;00m\n",
      "    \u001b[33m        an iterable of :class:`str`/:class:`bytes`, or a file-like object.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param headers:\u001b[39;49;00m\n",
      "    \u001b[33m        Dictionary of custom headers to send, such as User-Agent,\u001b[39;49;00m\n",
      "    \u001b[33m        If-None-Match, etc. If None, pool headers are used. If provided,\u001b[39;49;00m\n",
      "    \u001b[33m        these headers completely replace any pool-specific headers.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param retries:\u001b[39;49;00m\n",
      "    \u001b[33m        Configure the number of retries to allow before raising a\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`~urllib3.exceptions.MaxRetryError` exception.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m        Pass ``None`` to retry until you receive a response. Pass a\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`~urllib3.util.retry.Retry` object for fine-grained control\u001b[39;49;00m\n",
      "    \u001b[33m        over different types of retries.\u001b[39;49;00m\n",
      "    \u001b[33m        Pass an integer number to retry connection errors that many times,\u001b[39;49;00m\n",
      "    \u001b[33m        but no other types of errors. Pass zero to never retry.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m        If ``False``, then retries are disabled and any exception is raised\u001b[39;49;00m\n",
      "    \u001b[33m        immediately. Also, instead of raising a MaxRetryError on redirects,\u001b[39;49;00m\n",
      "    \u001b[33m        the redirect response will be returned.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param timeout:\u001b[39;49;00m\n",
      "    \u001b[33m        If specified, overrides the default timeout for this one\u001b[39;49;00m\n",
      "    \u001b[33m        request. It may be a float (in seconds) or an instance of\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`urllib3.util.Timeout`.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param chunked:\u001b[39;49;00m\n",
      "    \u001b[33m        If True, urllib3 will send the body using chunked transfer\u001b[39;49;00m\n",
      "    \u001b[33m        encoding. Otherwise, urllib3 will send the body using the standard\u001b[39;49;00m\n",
      "    \u001b[33m        content-length form. Defaults to False.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param response_conn:\u001b[39;49;00m\n",
      "    \u001b[33m        Set this to ``None`` if you will handle releasing the connection or\u001b[39;49;00m\n",
      "    \u001b[33m        set the connection to have the response release it.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param preload_content:\u001b[39;49;00m\n",
      "    \u001b[33m      If True, the response's body will be preloaded during construction.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param decode_content:\u001b[39;49;00m\n",
      "    \u001b[33m        If True, will attempt to decode the body based on the\u001b[39;49;00m\n",
      "    \u001b[33m        'content-encoding' header.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param enforce_content_length:\u001b[39;49;00m\n",
      "    \u001b[33m        Enforce content length checking. Body returned by server must match\u001b[39;49;00m\n",
      "    \u001b[33m        value of Content-Length header, if present. Otherwise, raise error.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.num_requests += \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        timeout_obj = \u001b[96mself\u001b[39;49;00m._get_timeout(timeout)\u001b[90m\u001b[39;49;00m\n",
      "        timeout_obj.start_connect()\u001b[90m\u001b[39;49;00m\n",
      "        conn.timeout = Timeout.resolve_default_timeout(timeout_obj.connect_timeout)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Trigger any extra validation we need to do.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[96mself\u001b[39;49;00m._validate_conn(conn)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m:466: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m:1095: in _validate_conn\n",
      "    \u001b[0mconn.connect()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m:652: in connect\n",
      "    \u001b[0msock_and_verified = _ssl_wrap_socket_and_match_hostname(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m:805: in _ssl_wrap_socket_and_match_hostname\n",
      "    \u001b[0mssl_sock = ssl_wrap_socket(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/util/ssl_.py\u001b[0m:465: in ssl_wrap_socket\n",
      "    \u001b[0mssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/util/ssl_.py\u001b[0m:509: in _ssl_wrap_socket_impl\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ssl_context.wrap_socket(sock, server_hostname=server_hostname)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/ssl.py\u001b[0m:501: in wrap_socket\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.sslsocket_class._create(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/ssl.py\u001b[0m:1074: in _create\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.do_handshake()\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>\n",
      "block = False\n",
      "\n",
      "    \u001b[0m\u001b[37m@_sslcopydoc\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mdo_handshake\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, block=\u001b[94mFalse\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m._check_connected()\u001b[90m\u001b[39;49;00m\n",
      "        timeout = \u001b[96mself\u001b[39;49;00m.gettimeout()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m timeout == \u001b[94m0.0\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m block:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m.settimeout(\u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mself\u001b[39;49;00m._sslobj.do_handshake()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           ssl.SSLZeroReturnError: TLS/SSL connection has been closed (EOF) (_ssl.c:1133)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/ssl.py\u001b[0m:1343: SSLZeroReturnError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "self = <urllib3.connectionpool.HTTPSConnectionPool object at 0x7f7e3ebe9eb0>\n",
      "method = 'POST'\n",
      "url = '/_/api/submission?user_key=YOUR_KEY_HERE&func_name=attention_activation'\n",
      "body = None\n",
      "headers = {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '0'}\n",
      "retries = Retry(total=0, connect=None, read=False, redirect=None, status=None)\n",
      "redirect = False, assert_same_host = False\n",
      "timeout = Timeout(connect=None, read=None, total=None), pool_timeout = None\n",
      "release_conn = False, chunked = False, body_pos = None, preload_content = False\n",
      "decode_content = False, response_kw = {}\n",
      "parsed_url = Url(scheme=None, auth=None, host=None, port=None, path='/_/api/submission', query='user_key=YOUR_KEY_HERE&func_name=attention_activation', fragment=None)\n",
      "destination_scheme = None, conn = None, release_this_conn = True\n",
      "http_tunnel_required = False, err = None, clean_exit = False\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92murlopen\u001b[39;49;00m(  \u001b[90m# type: ignore[override]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        method: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        url: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        body: _TYPE_BODY | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        headers: typing.Mapping[\u001b[96mstr\u001b[39;49;00m, \u001b[96mstr\u001b[39;49;00m] | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        retries: Retry | \u001b[96mbool\u001b[39;49;00m | \u001b[96mint\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        redirect: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        assert_same_host: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\u001b[90m\u001b[39;49;00m\n",
      "        pool_timeout: \u001b[96mint\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        release_conn: \u001b[96mbool\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        chunked: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        body_pos: _TYPE_BODY_POSITION | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        preload_content: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        decode_content: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        **response_kw: typing.Any,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> BaseHTTPResponse:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Get a connection from the pool and perform an HTTP request. This is the\u001b[39;49;00m\n",
      "    \u001b[33m    lowest level call for making a request, so you'll need to specify all\u001b[39;49;00m\n",
      "    \u001b[33m    the raw details.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    .. note::\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m       More commonly, it's appropriate to use a convenience method\u001b[39;49;00m\n",
      "    \u001b[33m       such as :meth:`request`.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    .. note::\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m       `release_conn` will only behave as expected if\u001b[39;49;00m\n",
      "    \u001b[33m       `preload_content=False` because we want to make\u001b[39;49;00m\n",
      "    \u001b[33m       `preload_content=False` the default behaviour someday soon without\u001b[39;49;00m\n",
      "    \u001b[33m       breaking backwards compatibility.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param method:\u001b[39;49;00m\n",
      "    \u001b[33m        HTTP request method (such as GET, POST, PUT, etc.)\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param url:\u001b[39;49;00m\n",
      "    \u001b[33m        The URL to perform the request on.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param body:\u001b[39;49;00m\n",
      "    \u001b[33m        Data to send in the request body, either :class:`str`, :class:`bytes`,\u001b[39;49;00m\n",
      "    \u001b[33m        an iterable of :class:`str`/:class:`bytes`, or a file-like object.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param headers:\u001b[39;49;00m\n",
      "    \u001b[33m        Dictionary of custom headers to send, such as User-Agent,\u001b[39;49;00m\n",
      "    \u001b[33m        If-None-Match, etc. If None, pool headers are used. If provided,\u001b[39;49;00m\n",
      "    \u001b[33m        these headers completely replace any pool-specific headers.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param retries:\u001b[39;49;00m\n",
      "    \u001b[33m        Configure the number of retries to allow before raising a\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`~urllib3.exceptions.MaxRetryError` exception.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m        If ``None`` (default) will retry 3 times, see ``Retry.DEFAULT``. Pass a\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`~urllib3.util.retry.Retry` object for fine-grained control\u001b[39;49;00m\n",
      "    \u001b[33m        over different types of retries.\u001b[39;49;00m\n",
      "    \u001b[33m        Pass an integer number to retry connection errors that many times,\u001b[39;49;00m\n",
      "    \u001b[33m        but no other types of errors. Pass zero to never retry.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m        If ``False``, then retries are disabled and any exception is raised\u001b[39;49;00m\n",
      "    \u001b[33m        immediately. Also, instead of raising a MaxRetryError on redirects,\u001b[39;49;00m\n",
      "    \u001b[33m        the redirect response will be returned.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param redirect:\u001b[39;49;00m\n",
      "    \u001b[33m        If True, automatically handle redirects (status codes 301, 302,\u001b[39;49;00m\n",
      "    \u001b[33m        303, 307, 308). Each redirect counts as a retry. Disabling retries\u001b[39;49;00m\n",
      "    \u001b[33m        will disable redirect, too.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param assert_same_host:\u001b[39;49;00m\n",
      "    \u001b[33m        If ``True``, will make sure that the host of the pool requests is\u001b[39;49;00m\n",
      "    \u001b[33m        consistent else will raise HostChangedError. When ``False``, you can\u001b[39;49;00m\n",
      "    \u001b[33m        use the pool on an HTTP proxy and request foreign hosts.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param timeout:\u001b[39;49;00m\n",
      "    \u001b[33m        If specified, overrides the default timeout for this one\u001b[39;49;00m\n",
      "    \u001b[33m        request. It may be a float (in seconds) or an instance of\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`urllib3.util.Timeout`.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param pool_timeout:\u001b[39;49;00m\n",
      "    \u001b[33m        If set and the pool is set to block=True, then this method will\u001b[39;49;00m\n",
      "    \u001b[33m        block for ``pool_timeout`` seconds and raise EmptyPoolError if no\u001b[39;49;00m\n",
      "    \u001b[33m        connection is available within the time period.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param bool preload_content:\u001b[39;49;00m\n",
      "    \u001b[33m        If True, the response's body will be preloaded into memory.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param bool decode_content:\u001b[39;49;00m\n",
      "    \u001b[33m        If True, will attempt to decode the body based on the\u001b[39;49;00m\n",
      "    \u001b[33m        'content-encoding' header.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param release_conn:\u001b[39;49;00m\n",
      "    \u001b[33m        If False, then the urlopen call will not release the connection\u001b[39;49;00m\n",
      "    \u001b[33m        back into the pool once a response is received (but will release if\u001b[39;49;00m\n",
      "    \u001b[33m        you read the entire contents of the response such as when\u001b[39;49;00m\n",
      "    \u001b[33m        `preload_content=True`). This is useful if you're not preloading\u001b[39;49;00m\n",
      "    \u001b[33m        the response's content immediately. You will need to call\u001b[39;49;00m\n",
      "    \u001b[33m        ``r.release_conn()`` on the response ``r`` to return the connection\u001b[39;49;00m\n",
      "    \u001b[33m        back into the pool. If None, it takes the value of ``preload_content``\u001b[39;49;00m\n",
      "    \u001b[33m        which defaults to ``True``.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param bool chunked:\u001b[39;49;00m\n",
      "    \u001b[33m        If True, urllib3 will send the body using chunked transfer\u001b[39;49;00m\n",
      "    \u001b[33m        encoding. Otherwise, urllib3 will send the body using the standard\u001b[39;49;00m\n",
      "    \u001b[33m        content-length form. Defaults to False.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param int body_pos:\u001b[39;49;00m\n",
      "    \u001b[33m        Position to seek to in file-like body in the event of a retry or\u001b[39;49;00m\n",
      "    \u001b[33m        redirect. Typically this won't need to be set because urllib3 will\u001b[39;49;00m\n",
      "    \u001b[33m        auto-populate the value when needed.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        parsed_url = parse_url(url)\u001b[90m\u001b[39;49;00m\n",
      "        destination_scheme = parsed_url.scheme\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m headers \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            headers = \u001b[96mself\u001b[39;49;00m.headers\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(retries, Retry):\u001b[90m\u001b[39;49;00m\n",
      "            retries = Retry.from_int(retries, redirect=redirect, default=\u001b[96mself\u001b[39;49;00m.retries)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m release_conn \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            release_conn = preload_content\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Check host\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m assert_same_host \u001b[95mand\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_same_host(url):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m HostChangedError(\u001b[96mself\u001b[39;49;00m, url, retries)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Ensure that the URL we're connecting to is properly encoded\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m url.startswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            url = to_str(_encode_target(url))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            url = to_str(parsed_url.url)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        conn = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Track whether `conn` needs to be released before\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# returning/raising/recursing. Update this variable if necessary, and\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# leave `release_conn` constant throughout the function. That way, if\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# the function recurses, the original value of `release_conn` will be\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# passed down into the recursive call, and its value will be respected.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# See issue #651 [1] for details.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# [1] <https://github.com/urllib3/urllib3/issues/651>\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        release_this_conn = release_conn\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        http_tunnel_required = connection_requires_http_tunnel(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.proxy, \u001b[96mself\u001b[39;49;00m.proxy_config, destination_scheme\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Merge the proxy headers. Only done when not using HTTP CONNECT. We\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# have to copy the headers dict so we can safely change it without those\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# changes being reflected in anyone else's copy.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m http_tunnel_required:\u001b[90m\u001b[39;49;00m\n",
      "            headers = headers.copy()  \u001b[90m# type: ignore[attr-defined]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            headers.update(\u001b[96mself\u001b[39;49;00m.proxy_headers)  \u001b[90m# type: ignore[union-attr]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Must keep the exception bound to a separate variable or else Python 3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# complains about UnboundLocalError.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        err = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Keep track of whether we cleanly exited the except block. This\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# ensures we do proper cleanup in finally.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        clean_exit = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Rewind body position, if needed. Record current position\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# for future rewinds in the event of a redirect/retry.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        body_pos = set_file_position(body, body_pos)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Request a connection from the queue.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            timeout_obj = \u001b[96mself\u001b[39;49;00m._get_timeout(timeout)\u001b[90m\u001b[39;49;00m\n",
      "            conn = \u001b[96mself\u001b[39;49;00m._get_conn(timeout=pool_timeout)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            conn.timeout = timeout_obj.connect_timeout  \u001b[90m# type: ignore[assignment]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Is this a closed/new connection that requires CONNECT tunnelling?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.proxy \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m http_tunnel_required \u001b[95mand\u001b[39;49;00m conn.is_closed:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[96mself\u001b[39;49;00m._prepare_proxy(conn)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mexcept\u001b[39;49;00m (BaseSSLError, \u001b[96mOSError\u001b[39;49;00m, SocketTimeout) \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[96mself\u001b[39;49;00m._raise_timeout(\u001b[90m\u001b[39;49;00m\n",
      "                        err=e, url=\u001b[96mself\u001b[39;49;00m.proxy.url, timeout_value=conn.timeout\u001b[90m\u001b[39;49;00m\n",
      "                    )\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mraise\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# If we're going to release the connection in ``finally:``, then\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# the response doesn't need to know about the connection. Otherwise\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# it will also try to release it and we'll have a double-release\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# mess.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            response_conn = conn \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m release_conn \u001b[94melse\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Make the request on the HTTPConnection object\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">           response = \u001b[96mself\u001b[39;49;00m._make_request(\u001b[90m\u001b[39;49;00m\n",
      "                conn,\u001b[90m\u001b[39;49;00m\n",
      "                method,\u001b[90m\u001b[39;49;00m\n",
      "                url,\u001b[90m\u001b[39;49;00m\n",
      "                timeout=timeout_obj,\u001b[90m\u001b[39;49;00m\n",
      "                body=body,\u001b[90m\u001b[39;49;00m\n",
      "                headers=headers,\u001b[90m\u001b[39;49;00m\n",
      "                chunked=chunked,\u001b[90m\u001b[39;49;00m\n",
      "                retries=retries,\u001b[90m\u001b[39;49;00m\n",
      "                response_conn=response_conn,\u001b[90m\u001b[39;49;00m\n",
      "                preload_content=preload_content,\u001b[90m\u001b[39;49;00m\n",
      "                decode_content=decode_content,\u001b[90m\u001b[39;49;00m\n",
      "                **response_kw,\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m:789: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <urllib3.connectionpool.HTTPSConnectionPool object at 0x7f7e3ebe9eb0>\n",
      "conn = <urllib3.connection.HTTPSConnection object at 0x7f7e3ec0b6d0>\n",
      "method = 'POST'\n",
      "url = '/_/api/submission?user_key=YOUR_KEY_HERE&func_name=attention_activation'\n",
      "body = None\n",
      "headers = {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '0'}\n",
      "retries = Retry(total=0, connect=None, read=False, redirect=None, status=None)\n",
      "timeout = Timeout(connect=None, read=None, total=None), chunked = False\n",
      "response_conn = <urllib3.connection.HTTPSConnection object at 0x7f7e3ec0b6d0>\n",
      "preload_content = False, decode_content = False, enforce_content_length = True\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_make_request\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        conn: BaseHTTPConnection,\u001b[90m\u001b[39;49;00m\n",
      "        method: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        url: \u001b[96mstr\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        body: _TYPE_BODY | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        headers: typing.Mapping[\u001b[96mstr\u001b[39;49;00m, \u001b[96mstr\u001b[39;49;00m] | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        retries: Retry | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\u001b[90m\u001b[39;49;00m\n",
      "        chunked: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        response_conn: BaseHTTPConnection | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        preload_content: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        decode_content: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        enforce_content_length: \u001b[96mbool\u001b[39;49;00m = \u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> BaseHTTPResponse:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Perform a request on a given urllib connection object taken from our\u001b[39;49;00m\n",
      "    \u001b[33m    pool.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param conn:\u001b[39;49;00m\n",
      "    \u001b[33m        a connection from one of our connection pools\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param method:\u001b[39;49;00m\n",
      "    \u001b[33m        HTTP request method (such as GET, POST, PUT, etc.)\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param url:\u001b[39;49;00m\n",
      "    \u001b[33m        The URL to perform the request on.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param body:\u001b[39;49;00m\n",
      "    \u001b[33m        Data to send in the request body, either :class:`str`, :class:`bytes`,\u001b[39;49;00m\n",
      "    \u001b[33m        an iterable of :class:`str`/:class:`bytes`, or a file-like object.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param headers:\u001b[39;49;00m\n",
      "    \u001b[33m        Dictionary of custom headers to send, such as User-Agent,\u001b[39;49;00m\n",
      "    \u001b[33m        If-None-Match, etc. If None, pool headers are used. If provided,\u001b[39;49;00m\n",
      "    \u001b[33m        these headers completely replace any pool-specific headers.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param retries:\u001b[39;49;00m\n",
      "    \u001b[33m        Configure the number of retries to allow before raising a\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`~urllib3.exceptions.MaxRetryError` exception.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m        Pass ``None`` to retry until you receive a response. Pass a\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`~urllib3.util.retry.Retry` object for fine-grained control\u001b[39;49;00m\n",
      "    \u001b[33m        over different types of retries.\u001b[39;49;00m\n",
      "    \u001b[33m        Pass an integer number to retry connection errors that many times,\u001b[39;49;00m\n",
      "    \u001b[33m        but no other types of errors. Pass zero to never retry.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m        If ``False``, then retries are disabled and any exception is raised\u001b[39;49;00m\n",
      "    \u001b[33m        immediately. Also, instead of raising a MaxRetryError on redirects,\u001b[39;49;00m\n",
      "    \u001b[33m        the redirect response will be returned.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param timeout:\u001b[39;49;00m\n",
      "    \u001b[33m        If specified, overrides the default timeout for this one\u001b[39;49;00m\n",
      "    \u001b[33m        request. It may be a float (in seconds) or an instance of\u001b[39;49;00m\n",
      "    \u001b[33m        :class:`urllib3.util.Timeout`.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param chunked:\u001b[39;49;00m\n",
      "    \u001b[33m        If True, urllib3 will send the body using chunked transfer\u001b[39;49;00m\n",
      "    \u001b[33m        encoding. Otherwise, urllib3 will send the body using the standard\u001b[39;49;00m\n",
      "    \u001b[33m        content-length form. Defaults to False.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param response_conn:\u001b[39;49;00m\n",
      "    \u001b[33m        Set this to ``None`` if you will handle releasing the connection or\u001b[39;49;00m\n",
      "    \u001b[33m        set the connection to have the response release it.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param preload_content:\u001b[39;49;00m\n",
      "    \u001b[33m      If True, the response's body will be preloaded during construction.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param decode_content:\u001b[39;49;00m\n",
      "    \u001b[33m        If True, will attempt to decode the body based on the\u001b[39;49;00m\n",
      "    \u001b[33m        'content-encoding' header.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param enforce_content_length:\u001b[39;49;00m\n",
      "    \u001b[33m        Enforce content length checking. Body returned by server must match\u001b[39;49;00m\n",
      "    \u001b[33m        value of Content-Length header, if present. Otherwise, raise error.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.num_requests += \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        timeout_obj = \u001b[96mself\u001b[39;49;00m._get_timeout(timeout)\u001b[90m\u001b[39;49;00m\n",
      "        timeout_obj.start_connect()\u001b[90m\u001b[39;49;00m\n",
      "        conn.timeout = Timeout.resolve_default_timeout(timeout_obj.connect_timeout)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Trigger any extra validation we need to do.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m._validate_conn(conn)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m (SocketTimeout, BaseSSLError) \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mself\u001b[39;49;00m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# _validate_conn() starts the connection to an HTTPS proxy\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# so we need to wrap errors with 'ProxyError' here too.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mOSError\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            NewConnectionError,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mTimeoutError\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            BaseSSLError,\u001b[90m\u001b[39;49;00m\n",
      "            CertificateError,\u001b[90m\u001b[39;49;00m\n",
      "            SSLError,\u001b[90m\u001b[39;49;00m\n",
      "        ) \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      "            new_e: \u001b[96mException\u001b[39;49;00m = e\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(e, (BaseSSLError, CertificateError)):\u001b[90m\u001b[39;49;00m\n",
      "                new_e = SSLError(e)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# If the connection didn't successfully connect to it's proxy\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# then there\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                new_e, (\u001b[96mOSError\u001b[39;49;00m, NewConnectionError, \u001b[96mTimeoutError\u001b[39;49;00m, SSLError)\u001b[90m\u001b[39;49;00m\n",
      "            ) \u001b[95mand\u001b[39;49;00m (conn \u001b[95mand\u001b[39;49;00m conn.proxy \u001b[95mand\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m conn.has_connected_to_proxy):\u001b[90m\u001b[39;49;00m\n",
      "                new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m new_e\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           urllib3.exceptions.SSLError: TLS/SSL connection has been closed (EOF) (_ssl.c:1133)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m:490: SSLError\n",
      "\n",
      "\u001b[33mThe above exception was the direct cause of the following exception:\u001b[0m\n",
      "\n",
      "self = <requests.adapters.HTTPAdapter object at 0x7f7eeffd21f0>\n",
      "request = <PreparedRequest [POST]>, stream = False\n",
      "timeout = Timeout(connect=None, read=None, total=None), verify = False\n",
      "cert = None, proxies = OrderedDict()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92msend\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, request, stream=\u001b[94mFalse\u001b[39;49;00m, timeout=\u001b[94mNone\u001b[39;49;00m, verify=\u001b[94mTrue\u001b[39;49;00m, cert=\u001b[94mNone\u001b[39;49;00m, proxies=\u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Sends PreparedRequest object. Returns Response object.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\u001b[39;49;00m\n",
      "    \u001b[33m    :param stream: (optional) Whether to stream the request content.\u001b[39;49;00m\n",
      "    \u001b[33m    :param timeout: (optional) How long to wait for the server to send\u001b[39;49;00m\n",
      "    \u001b[33m        data before giving up, as a float, or a :ref:`(connect timeout,\u001b[39;49;00m\n",
      "    \u001b[33m        read timeout) <timeouts>` tuple.\u001b[39;49;00m\n",
      "    \u001b[33m    :type timeout: float or tuple or urllib3 Timeout object\u001b[39;49;00m\n",
      "    \u001b[33m    :param verify: (optional) Either a boolean, in which case it controls whether\u001b[39;49;00m\n",
      "    \u001b[33m        we verify the server's TLS certificate, or a string, in which case it\u001b[39;49;00m\n",
      "    \u001b[33m        must be a path to a CA bundle to use\u001b[39;49;00m\n",
      "    \u001b[33m    :param cert: (optional) Any user-provided SSL certificate to be trusted.\u001b[39;49;00m\n",
      "    \u001b[33m    :param proxies: (optional) The proxies dictionary to apply to the request.\u001b[39;49;00m\n",
      "    \u001b[33m    :rtype: requests.Response\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            conn = \u001b[96mself\u001b[39;49;00m.get_connection_with_tls_context(\u001b[90m\u001b[39;49;00m\n",
      "                request, verify, proxies=proxies, cert=cert\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m LocationValueError \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m InvalidURL(e, request=request)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.cert_verify(conn, request.url, verify, cert)\u001b[90m\u001b[39;49;00m\n",
      "        url = \u001b[96mself\u001b[39;49;00m.request_url(request, proxies)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.add_headers(\u001b[90m\u001b[39;49;00m\n",
      "            request,\u001b[90m\u001b[39;49;00m\n",
      "            stream=stream,\u001b[90m\u001b[39;49;00m\n",
      "            timeout=timeout,\u001b[90m\u001b[39;49;00m\n",
      "            verify=verify,\u001b[90m\u001b[39;49;00m\n",
      "            cert=cert,\u001b[90m\u001b[39;49;00m\n",
      "            proxies=proxies,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        chunked = \u001b[95mnot\u001b[39;49;00m (request.body \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mContent-Length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m request.headers)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(timeout, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                connect, read = timeout\u001b[90m\u001b[39;49;00m\n",
      "                timeout = TimeoutSauce(connect=connect, read=read)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid timeout \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtimeout\u001b[33m}\u001b[39;49;00m\u001b[33m. Pass a (connect, read) timeout tuple, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mor a single float to set both timeouts to the same value.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(timeout, TimeoutSauce):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            timeout = TimeoutSauce(connect=timeout, read=timeout)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           resp = conn.urlopen(\u001b[90m\u001b[39;49;00m\n",
      "                method=request.method,\u001b[90m\u001b[39;49;00m\n",
      "                url=url,\u001b[90m\u001b[39;49;00m\n",
      "                body=request.body,\u001b[90m\u001b[39;49;00m\n",
      "                headers=request.headers,\u001b[90m\u001b[39;49;00m\n",
      "                redirect=\u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                assert_same_host=\u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                preload_content=\u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                decode_content=\u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                retries=\u001b[96mself\u001b[39;49;00m.max_retries,\u001b[90m\u001b[39;49;00m\n",
      "                timeout=timeout,\u001b[90m\u001b[39;49;00m\n",
      "                chunked=chunked,\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/requests/adapters.py\u001b[0m:667: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m:843: in urlopen\n",
      "    \u001b[0mretries = retries.increment(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = Retry(total=0, connect=None, read=False, redirect=None, status=None)\n",
      "method = 'POST'\n",
      "url = '/_/api/submission?user_key=YOUR_KEY_HERE&func_name=attention_activation'\n",
      "response = None\n",
      "error = SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1133)'))\n",
      "_pool = <urllib3.connectionpool.HTTPSConnectionPool object at 0x7f7e3ebe9eb0>\n",
      "_stacktrace = <traceback object at 0x7f7e406deec0>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mincrement\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        method: \u001b[96mstr\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        url: \u001b[96mstr\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        response: BaseHTTPResponse | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        error: \u001b[96mException\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        _pool: ConnectionPool | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        _stacktrace: TracebackType | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Self:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Return a new Retry object with incremented retry counters.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param response: A response object, or None, if the server did not\u001b[39;49;00m\n",
      "    \u001b[33m        return a response.\u001b[39;49;00m\n",
      "    \u001b[33m    :type response: :class:`~urllib3.response.BaseHTTPResponse`\u001b[39;49;00m\n",
      "    \u001b[33m    :param Exception error: An error encountered during the request, or\u001b[39;49;00m\n",
      "    \u001b[33m        None if the response was received successfully.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :return: A new ``Retry`` object.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.total \u001b[95mis\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m error:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Disabled, indicate to re-raise the error.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m reraise(\u001b[96mtype\u001b[39;49;00m(error), error, _stacktrace)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        total = \u001b[96mself\u001b[39;49;00m.total\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m total \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            total -= \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        connect = \u001b[96mself\u001b[39;49;00m.connect\u001b[90m\u001b[39;49;00m\n",
      "        read = \u001b[96mself\u001b[39;49;00m.read\u001b[90m\u001b[39;49;00m\n",
      "        redirect = \u001b[96mself\u001b[39;49;00m.redirect\u001b[90m\u001b[39;49;00m\n",
      "        status_count = \u001b[96mself\u001b[39;49;00m.status\u001b[90m\u001b[39;49;00m\n",
      "        other = \u001b[96mself\u001b[39;49;00m.other\u001b[90m\u001b[39;49;00m\n",
      "        cause = \u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        status = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        redirect_location = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m error \u001b[95mand\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._is_connection_error(error):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Connect retry?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m connect \u001b[95mis\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m reraise(\u001b[96mtype\u001b[39;49;00m(error), error, _stacktrace)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m connect \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                connect -= \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melif\u001b[39;49;00m error \u001b[95mand\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._is_read_error(error):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Read retry?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m read \u001b[95mis\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m method \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._is_method_retryable(method):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m reraise(\u001b[96mtype\u001b[39;49;00m(error), error, _stacktrace)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melif\u001b[39;49;00m read \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                read -= \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melif\u001b[39;49;00m error:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Other retry?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m other \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                other -= \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melif\u001b[39;49;00m response \u001b[95mand\u001b[39;49;00m response.get_redirect_location():\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Redirect retry?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m redirect \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                redirect -= \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            cause = \u001b[33m\"\u001b[39;49;00m\u001b[33mtoo many redirects\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            response_redirect_location = response.get_redirect_location()\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m response_redirect_location:\u001b[90m\u001b[39;49;00m\n",
      "                redirect_location = response_redirect_location\u001b[90m\u001b[39;49;00m\n",
      "            status = response.status\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Incrementing because of a server error like a 500 in\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# status_forcelist and the given method is in the allowed_methods\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            cause = ResponseError.GENERIC_ERROR\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m response \u001b[95mand\u001b[39;49;00m response.status:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m status_count \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    status_count -= \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                cause = ResponseError.SPECIFIC_ERROR.format(status_code=response.status)\u001b[90m\u001b[39;49;00m\n",
      "                status = response.status\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        history = \u001b[96mself\u001b[39;49;00m.history + (\u001b[90m\u001b[39;49;00m\n",
      "            RequestHistory(method, url, error, status, redirect_location),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        new_retry = \u001b[96mself\u001b[39;49;00m.new(\u001b[90m\u001b[39;49;00m\n",
      "            total=total,\u001b[90m\u001b[39;49;00m\n",
      "            connect=connect,\u001b[90m\u001b[39;49;00m\n",
      "            read=read,\u001b[90m\u001b[39;49;00m\n",
      "            redirect=redirect,\u001b[90m\u001b[39;49;00m\n",
      "            status=status_count,\u001b[90m\u001b[39;49;00m\n",
      "            other=other,\u001b[90m\u001b[39;49;00m\n",
      "            history=history,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m new_retry.is_exhausted():\u001b[90m\u001b[39;49;00m\n",
      "            reason = error \u001b[95mor\u001b[39;49;00m ResponseError(cause)\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m MaxRetryError(_pool, url, reason) \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mreason\u001b[39;49;00m  \u001b[90m# type: ignore[arg-type]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mugrade.dlsyscourse.org', port=443): Max retries exceeded with url: /_/api/submission?user_key=YOUR_KEY_HERE&func_name=attention_activation (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1133)')))\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/urllib3/util/retry.py\u001b[0m:519: MaxRetryError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "pyfuncitem = <Function submit_attention_activation>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.hookimpl(hookwrapper=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpytest_pyfunc_call\u001b[39;49;00m(pyfuncitem):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m## prior to test, initialize submission\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mglobal\u001b[39;49;00m _values, _submission_key, _errors\u001b[90m\u001b[39;49;00m\n",
      "        _values = []\u001b[90m\u001b[39;49;00m\n",
      "        _errors = \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        func_name = pyfuncitem.name[\u001b[94m7\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_OP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33msubmit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           _submission_key = start_submission(func_name)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/mugrade/mugrade.py\u001b[0m:164: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/mugrade/mugrade.py\u001b[0m:95: in start_submission\n",
      "    \u001b[0mresponse = requests.post(server_url + \u001b[33m\"\u001b[39;49;00m\u001b[33msubmission\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/requests/api.py\u001b[0m:115: in post\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m request(\u001b[33m\"\u001b[39;49;00m\u001b[33mpost\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, url, data=data, json=json, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/requests/api.py\u001b[0m:59: in request\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m session.request(method=method, url=url, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/requests/sessions.py\u001b[0m:589: in request\n",
      "    \u001b[0mresp = \u001b[96mself\u001b[39;49;00m.send(prep, **send_kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/requests/sessions.py\u001b[0m:703: in send\n",
      "    \u001b[0mr = adapter.send(request, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <requests.adapters.HTTPAdapter object at 0x7f7eeffd21f0>\n",
      "request = <PreparedRequest [POST]>, stream = False\n",
      "timeout = Timeout(connect=None, read=None, total=None), verify = False\n",
      "cert = None, proxies = OrderedDict()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92msend\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, request, stream=\u001b[94mFalse\u001b[39;49;00m, timeout=\u001b[94mNone\u001b[39;49;00m, verify=\u001b[94mTrue\u001b[39;49;00m, cert=\u001b[94mNone\u001b[39;49;00m, proxies=\u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Sends PreparedRequest object. Returns Response object.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\u001b[39;49;00m\n",
      "    \u001b[33m    :param stream: (optional) Whether to stream the request content.\u001b[39;49;00m\n",
      "    \u001b[33m    :param timeout: (optional) How long to wait for the server to send\u001b[39;49;00m\n",
      "    \u001b[33m        data before giving up, as a float, or a :ref:`(connect timeout,\u001b[39;49;00m\n",
      "    \u001b[33m        read timeout) <timeouts>` tuple.\u001b[39;49;00m\n",
      "    \u001b[33m    :type timeout: float or tuple or urllib3 Timeout object\u001b[39;49;00m\n",
      "    \u001b[33m    :param verify: (optional) Either a boolean, in which case it controls whether\u001b[39;49;00m\n",
      "    \u001b[33m        we verify the server's TLS certificate, or a string, in which case it\u001b[39;49;00m\n",
      "    \u001b[33m        must be a path to a CA bundle to use\u001b[39;49;00m\n",
      "    \u001b[33m    :param cert: (optional) Any user-provided SSL certificate to be trusted.\u001b[39;49;00m\n",
      "    \u001b[33m    :param proxies: (optional) The proxies dictionary to apply to the request.\u001b[39;49;00m\n",
      "    \u001b[33m    :rtype: requests.Response\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            conn = \u001b[96mself\u001b[39;49;00m.get_connection_with_tls_context(\u001b[90m\u001b[39;49;00m\n",
      "                request, verify, proxies=proxies, cert=cert\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m LocationValueError \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m InvalidURL(e, request=request)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.cert_verify(conn, request.url, verify, cert)\u001b[90m\u001b[39;49;00m\n",
      "        url = \u001b[96mself\u001b[39;49;00m.request_url(request, proxies)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.add_headers(\u001b[90m\u001b[39;49;00m\n",
      "            request,\u001b[90m\u001b[39;49;00m\n",
      "            stream=stream,\u001b[90m\u001b[39;49;00m\n",
      "            timeout=timeout,\u001b[90m\u001b[39;49;00m\n",
      "            verify=verify,\u001b[90m\u001b[39;49;00m\n",
      "            cert=cert,\u001b[90m\u001b[39;49;00m\n",
      "            proxies=proxies,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        chunked = \u001b[95mnot\u001b[39;49;00m (request.body \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mContent-Length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m request.headers)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(timeout, \u001b[96mtuple\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                connect, read = timeout\u001b[90m\u001b[39;49;00m\n",
      "                timeout = TimeoutSauce(connect=connect, read=read)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid timeout \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtimeout\u001b[33m}\u001b[39;49;00m\u001b[33m. Pass a (connect, read) timeout tuple, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mor a single float to set both timeouts to the same value.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(timeout, TimeoutSauce):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            timeout = TimeoutSauce(connect=timeout, read=timeout)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            resp = conn.urlopen(\u001b[90m\u001b[39;49;00m\n",
      "                method=request.method,\u001b[90m\u001b[39;49;00m\n",
      "                url=url,\u001b[90m\u001b[39;49;00m\n",
      "                body=request.body,\u001b[90m\u001b[39;49;00m\n",
      "                headers=request.headers,\u001b[90m\u001b[39;49;00m\n",
      "                redirect=\u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                assert_same_host=\u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                preload_content=\u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                decode_content=\u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                retries=\u001b[96mself\u001b[39;49;00m.max_retries,\u001b[90m\u001b[39;49;00m\n",
      "                timeout=timeout,\u001b[90m\u001b[39;49;00m\n",
      "                chunked=chunked,\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m (ProtocolError, \u001b[96mOSError\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m err:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mConnectionError\u001b[39;49;00m(err, request=request)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m MaxRetryError \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(e.reason, ConnectTimeoutError):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# TODO: Remove this in 3.0.0: see #2811\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(e.reason, NewConnectionError):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mraise\u001b[39;49;00m ConnectTimeout(e, request=request)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(e.reason, ResponseError):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m RetryError(e, request=request)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(e.reason, _ProxyError):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m ProxyError(e, request=request)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(e.reason, _SSLError):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is for urllib3 v1.22 and later.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[94mraise\u001b[39;49;00m SSLError(e, request=request)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               requests.exceptions.SSLError: HTTPSConnectionPool(host='mugrade.dlsyscourse.org', port=443): Max retries exceeded with url: /_/api/submission?user_key=YOUR_KEY_HERE&func_name=attention_activation (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1133)')))\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/site-packages/requests/adapters.py\u001b[0m:698: SSLError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1msubmit_attention_activation\u001b[0m - requests.exceptions.SSLError: HTTPSConnectionPool(host='mugrade.dlsyscourse...\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[31m in 32.57s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"YOUR_KEY_HERE\" -k \"attention_activation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65aea6",
   "metadata": {},
   "source": [
    "## Part 2 Implementing the Self-Attention Layer with trainable parameters\n",
    "\n",
    "In this subproblem, you will use the `MultiHeadAttention` class you just implemented, and wrap it in a subclass of `Module` called `AttentionLayer` in `python/needle/nn/nn_transformer.py`. \n",
    "\n",
    "This layer implements the self-attention with prenorm (when k, and v are None in the `self.forward` call) and cross-attention (when k and v are present in the `self.forward` call). We have provided skeleton code with the appropriate layer attributes defined. Your job is to write the forward pass of the `AttentionLayer`. Note that you are implementing multi-head attention, where the number of attention heads is given by the `self.num_head` attribute of the `AttentionLayer` class.\n",
    "\n",
    "Given inputs $Q \\in R^\\mathcal{B \\times T \\times D'}$, keys $K \\in R^\\mathcal{B \\times T \\times D'}$, and values $V \\in R^\\mathcal{B \\times T \\times D'}$ where $B$ is the batch size, $T$ is the sequence length, and $D'$ is the embedding dimension. This layer performs the following computation sequentially:\n",
    "\n",
    "(1) map queries, key, and values to heads.\n",
    "\n",
    "<p style=\"text-align: center;\">$Q' = \\text{LayerNorm}_q (Q) \\; W_q$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$K' = \\text{LayerNorm}_k (K) \\; W_k$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$V' = \\text{LayerNorm}_v (V) \\; W_v$</p>\n",
    "\n",
    "where $\\text{LayerNorm}_q , \\text{LayerNorm}_k, \\text{LayerNorm}_v $ are the prenorm `self.prenorm_q`, `self.prenorm_k` and `self.prenorm_v` respectively.\n",
    "\n",
    "(2) unravel heads from the channels axis.\n",
    "\n",
    "<p style=\"text-align: center;\">$Q' \\in R^{B \\times T \\times (HD)} \\to Q' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$K' \\in R^{B \\times T \\times (HD)} \\to K' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$V' \\in R^{B \\times T \\times (HD)} \\to V' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "where $H$ and $D$ are `self.num_head` and `self.head_dim` respectively.\n",
    "\n",
    "(3) compute the multi-head attention activation.\n",
    "\n",
    "<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q' (K')^T}{\\sqrt{D}}) V'$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$X \\in R^{B \\times H \\times T \\times D} \\to X \\in R^{B \\times T \\times H \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$X \\in R^{B \\times T \\times H \\times D} \\to X \\in R^{B \\times T \\times (HD)}$</p>\n",
    "\n",
    "The last two steps do a transpose and then reshape to get the hidden states to be the correct shape.\n",
    "\n",
    "(4) project back to the input space of the layer with `self.out_projection`\n",
    "\n",
    "<p style=\"text-align: center;\">$X' = X \\; W_o$</p>\n",
    "\n",
    "Your goal in this part is to return $X$ in the `self.forward` call of `AttentionLayer`. For debugging, you may capture the `probs` variable returned by the inner `MultiHeadAttention` module and store it in an attribute such as `self.probs` of the attention layer.\n",
    "\n",
    "Once finished, you may test your layer with the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b2fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.19, pytest-8.3.1, pluggy-1.5.0 -- /root/miniconda3/envs/LightTorch/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /root/workspace/LightTorch/archive/hw4_extra\n",
      "plugins: anyio-4.4.0\n",
      "collected 112 items / 80 deselected / 32 selected                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m32 passed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[32m in 2.01s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"attention_layer\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR_KEY_HERE\" -k \"attention_layer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8fb30",
   "metadata": {},
   "source": [
    "## Part 3 Implementing a prenorm residual Transformer Layer\n",
    "\n",
    "You now have all the parts necessary to build a full Transformer by this point. In this subproblem, you will assemble the attention layer with a feedforward network into a stackable residual block. We have provided starter code in the `TransformerLayer` class. \n",
    "\n",
    "You will need to define the necessary class attributes in the `self.__init__` call of the module `TransformerLayer`, and fill in the forward pass in `self.forward`. Your transformer layer should support dropout applied to $X'$ from the previous step before adding a residual connection. Implement the following pseudocode of the layer, properly handling the intermediate tensor shapes:\n",
    "\n",
    "x - current sequence of hidden states\n",
    "\n",
    "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Attention}(x))$</p>\n",
    "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Linear}_{2}(\\text{Dropout}(\\text{ReLU}(\\text{Linear}_{1}(\\text{LayerNorm1d}(x))))))$</p>\n",
    "\n",
    "For the MLP, there are two Linear layers $\\text{Linear}_{1}$ and $\\text{Linear}_{2}$:\n",
    "- $\\text{Linear}_{1}$: input shape `q_features`, output shape `hidden_size`\n",
    "- $\\text{Linear}_{2}$: input shape `hidden_size`, output shape `q_features`\n",
    "\n",
    "Once finished, run the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e0fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.19, pytest-8.3.1, pluggy-1.5.0 -- /root/miniconda3/envs/LightTorch/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /root/workspace/LightTorch/archive/hw4_extra\n",
      "plugins: anyio-4.4.0\n",
      "collected 112 items / 80 deselected / 32 selected                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m32 passed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[32m in 2.03s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"transformer_layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74a6ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.19, pytest-8.3.1, pluggy-1.5.0\n",
      "rootdir: /root/workspace/LightTorch/archive/hw4_extra\n",
      "plugins: anyio-4.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
      "collected 4 items / 3 deselected / 1 selected                                  \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py ^C\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"YOUR_KEY_HERE\" -k \"transformer_layer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e78953",
   "metadata": {},
   "source": [
    "## Part 4 Implementing the Transformer model\n",
    "\n",
    "In this subsection, you will compose the residual transformer layers you implemented in the previous part to build the full Transformer model. Fill in the code in the `Transformer` class by defining a set of `num_layers` `TransformerLayer` modules with the appropriat parameters passed in from the parent `Transformer` class. Then, implement the `self.forward` call of the `Transformer`. \n",
    "\n",
    "As is, your current Transformer layers are permutation-invariant, and cannot tell which position each token is in the sequence. To break this symmetry, you will add a positional embedding to your Transformer.\n",
    "\n",
    "The original Transformer paper uses sinusoidal positional embeddings, and then adds to the input embeddings before the first `TransformerLayer`. These work well, but a more common strategy in modern Transformers is to learn the positional embeddings. \n",
    "\n",
    "To do this, you should use `needle.nn.Embedding`. In your Transformer implementation, create a learnable positional encoding using `needle.nn.Embedding` from homework 4, with `num_embeddings` set as `sequence_len`. Given an input sequence, you should create a tensor that has the timestep id of each token in the sequence (timesteps have increasing value, representing the position of a token in time), and use it like a word id. \n",
    "\n",
    "Last, add the created positional encoding to the input token embeddings before your transformer layers.\n",
    "\n",
    "Once complete, submit the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec5fb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.19, pytest-8.3.1, pluggy-1.5.0 -- /root/miniconda3/envs/LightTorch/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /root/workspace/LightTorch/archive/hw4_extra\n",
      "plugins: anyio-4.4.0\n",
      "collected 112 items / 80 deselected / 32 selected                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-5-27-64-2-8-32-False-0.0-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-False-0.0-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[-5.59489846e-01, -2.19426775e+00,  6.14241123e+00, ...,\n",
      "          2.49509168e+00, -5.09634686e+00,  9.5644736...43649793e+00,  3.40526509e+00, ...,\n",
      "          1.90174413e+00, -4.33436584e+00, -1.54363143e+00]]],\n",
      "      dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-False-0.0-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c15829460>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[ 1.27858424e+00, -4.19090331e-01,  8.84248495e-01, ...,\n",
      "          2.01707512e-01, -2.82128423e-01,  6.5203905...75588262e+00,  1.69019848e-01, ...,\n",
      "          1.03529727e+00,  5.99163651e-01, -1.33877492e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0ac2d704c0>, array([[[ 1.27858424e+00, -4.19090331e-01,  8.84248495...3649793e+00,  3.40526509e+00, ...,\n",
      "          1.90174413e+00, -4.33436584e+00, -1.54363143e+00]]],\n",
      "      dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 11.499636\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 1325.2479\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 1.278584e+00, -4.190903e-01,  8.842485e-01, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     2.017075e-01, -2.821284e-01,  6.520391e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.149984e+00, -2.646872e-01,  1.720497e+00, ...,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[-5.594898e-01, -2.194268e+00,  6.142411e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     2.495092e+00, -5.096347e+00,  9.564474e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-2.803165e+00, -3.013344e+00,  3.758643e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0ac2d704c0>, array([[[ 1.27858424e+00, -4.19090331e-01,  8.84248495...3649793e+00,  3.40526509e+00, ...,\n",
      "          1.90174413e+00, -4.33436584e+00, -1.54363143e+00]]],\n",
      "      dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-11-27-64-2-8-32-False-0.0-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-False-0.0-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[-1.4169730e+00, -3.1259732e+00,  6.5792274e+00, ...,\n",
      "          8.1491079e+00,  2.1011806e+00, -6.5926986e+00]...49e+00, -2.4207897e+00,  3.6112609e+00, ...,\n",
      "          7.0434079e+00, -7.7669609e-01, -1.2579595e+00]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-False-0.0-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d1b490>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[ 0.6291168 , -0.51622045,  1.0636867 , ..., -0.12832734,\n",
      "         -1.0119821 , -2.217086  ],\n",
      "        [-0.3328...\n",
      "        [-0.32706493,  1.967925  ,  0.3595861 , ...,  0.19388613,\n",
      "          1.3994869 ,  0.65696096]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0ac2d705e0>, array([[[ 0.6291168 , -0.51622045,  1.0636867 , ..., -...9e+00, -2.4207897e+00,  3.6112609e+00, ...,\n",
      "          7.0434079e+00, -7.7669609e-01, -1.2579595e+00]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 10.522881\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 1600.6941\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 0.629117, -0.51622 ,  1.063687, ..., -0.128327, -1.011982,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -2.217086],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.332841,  1.152816,  0.949509, ...,  0.775403, -0.032665,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[-1.416973e+00, -3.125973e+00,  6.579227e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     8.149108e+00,  2.101181e+00, -6.592699e+00],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.830268e+00, -7.977794e-01,  8.756701e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0ac2d705e0>, array([[[ 0.6291168 , -0.51622045,  1.0636867 , ..., -...9e+00, -2.4207897e+00,  3.6112609e+00, ...,\n",
      "          7.0434079e+00, -7.7669609e-01, -1.2579595e+00]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-5-27-64-4-8-32-False-0.0-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-False-0.0-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 3.659356  ,  0.68787265,  2.3118196 , ...,  3.2671494 ,\n",
      "         -2.709228  ,  4.820525  ],\n",
      "        [ 0.5323...\n",
      "        [ 6.480134  ,  1.5321801 , -0.2689309 , ...,  4.1558657 ,\n",
      "          0.5391756 , -3.4294152 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-False-0.0-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d3aeb0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[ 1.1966802 ,  1.404472  ,  1.0587273 , ...,  0.8214378 ,\n",
      "         -1.5532106 , -0.39313465],\n",
      "        [ 0.6073...\n",
      "        [-1.4070046 ,  1.0843732 ,  0.44417867, ...,  0.22686522,\n",
      "          0.50010866, -1.9926015 ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0ac2d70670>, array([[[ 1.1966802 ,  1.404472  ,  1.0587273 , ...,  ...        [ 6.480134  ,  1.5321801 , -0.2689309 , ...,  4.1558657 ,\n",
      "          0.5391756 , -3.4294152 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 16.290731\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 86.14811\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 1.19668 ,  1.404472,  1.058727, ...,  0.821438, -1.553211,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.393135],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.607399,  0.888674,  2.190732, ...,  0.690429, -1.704219,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 3.659356,  0.687873,  2.31182 , ...,  3.267149, -2.709228,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     4.820525],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.532353, -2.781026,  0.51681 , ...,  4.018684, -6.554438,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0ac2d70670>, array([[[ 1.1966802 ,  1.404472  ,  1.0587273 , ...,  ...        [ 6.480134  ,  1.5321801 , -0.2689309 , ...,  4.1558657 ,\n",
      "          0.5391756 , -3.4294152 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-11-27-64-4-8-32-False-0.0-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-False-0.0-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[-2.698675  , -3.5962393 ,  2.5475898 , ...,  8.575455  ,\n",
      "          5.4873857 , -0.9009913 ],\n",
      "        [-2.6018...\n",
      "        [ 1.9172084 , -5.8891587 ,  2.4184527 , ...,  5.9232826 ,\n",
      "          2.312411  ,  0.98988545]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-False-0.0-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d0ef70>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[-0.42570978,  1.7394766 ,  0.8601769 , ...,  1.1444633 ,\n",
      "         -0.7816328 ,  0.5242442 ],\n",
      "        [ 2.2220...\n",
      "        [-0.46020487,  0.7741473 ,  0.711359  , ...,  0.15940872,\n",
      "         -0.07540239,  1.907063  ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121ce3a0>, array([[[-0.42570978,  1.7394766 ,  0.8601769 , ...,  ...        [ 1.9172084 , -5.8891587 ,  2.4184527 , ...,  5.9232826 ,\n",
      "          2.312411  ,  0.98988545]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 13.273764\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 262.43073\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.42571 ,  1.739477,  0.860177, ...,  1.144463, -0.781633,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     0.524244],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 2.222084,  0.400267,  0.166152, ...,  0.590902, -1.03512 ,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[-2.698675, -3.596239,  2.54759 , ...,  8.575455,  5.487386,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.900991],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-2.601813, -6.391169,  7.881776, ...,  2.094326, -2.067447,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121ce3a0>, array([[[-0.42570978,  1.7394766 ,  0.8601769 , ...,  ...        [ 1.9172084 , -5.8891587 ,  2.4184527 , ...,  5.9232826 ,\n",
      "          2.312411  ,  0.98988545]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8] _____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-5-27-64-2-8-32-True-0.0-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-True-0.0-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  1.6476442 ,  -1.8613683 ,   3.2609386 , ...,   1.4043767 ,\n",
      "          -1.764772  ,   0.53275794],\n",
      "        [ ...   [ -2.4991958 ,   2.21719   ,   1.1389767 , ...,   0.9479253 ,\n",
      "          -5.7012367 ,  -1.9912137 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-True-0.0-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d83c40>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[-0.5885171 ,  0.59811425,  0.8724936 , ...,  0.51767856,\n",
      "          0.13866618, -0.06546182],\n",
      "        [ 0.3603...\n",
      "        [ 0.99254066, -1.6418678 ,  0.27018592, ...,  1.0907835 ,\n",
      "          0.3618097 , -1.2383053 ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121ce790>, array([[[-0.5885171 ,  0.59811425,  0.8724936 , ...,  ...  [ -2.4991958 ,   2.21719   ,   1.1389767 , ...,   0.9479253 ,\n",
      "          -5.7012367 ,  -1.9912137 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 12.618838\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 377.06433\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.588517,  0.598114,  0.872494, ...,  0.517679,  0.138666,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.065462],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.360305, -0.377494,  0.790841, ...,  1.554729, -0.574109,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  1.647644,  -1.861368,   3.260939, ...,   1.404377,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     -1.764772,   0.532758],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ -4.428632,   0.331762,   3.801016, ...,   2.275624,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121ce790>, array([[[-0.5885171 ,  0.59811425,  0.8724936 , ...,  ...  [ -2.4991958 ,   2.21719   ,   1.1389767 , ...,   0.9479253 ,\n",
      "          -5.7012367 ,  -1.9912137 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-11-27-64-2-8-32-True-0.0-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-True-0.0-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  2.6538882 ,  -2.3525443 ,   3.5054457 , ...,   1.8293397 ,\n",
      "          10.294293  ,  -4.634383  ],\n",
      "        [ ...   [  3.6802502 ,  -1.5486119 ,   3.407357  , ...,   8.523506  ,\n",
      "          -0.30161297,  -0.37603542]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-True-0.0-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c12209df0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[-1.3267026 ,  0.17535046,  0.5994783 , ...,  0.20292987,\n",
      "         -0.35895693, -0.17188787],\n",
      "        [-0.7986...\n",
      "        [-0.47717378,  1.6211251 , -0.09050179, ..., -0.11199523,\n",
      "          0.9802409 ,  0.9043474 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12185310>, array([[[-1.3267026 ,  0.17535046,  0.5994783 , ...,  ...  [  3.6802502 ,  -1.5486119 ,   3.407357  , ...,   8.523506  ,\n",
      "          -0.30161297,  -0.37603542]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 12.035321\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 265.20737\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.326703,  0.17535 ,  0.599478, ...,  0.20293 , -0.358957,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.171888],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.798601,  0.050449, -0.81543 , ...,  0.352967, -0.525187,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  2.653888,  -2.352544,   3.505446, ...,   1.82934 ,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     10.294293,  -4.634383],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  2.769305,  -2.337575,   3.823558, ...,   2.624812,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12185310>, array([[[-1.3267026 ,  0.17535046,  0.5994783 , ...,  ...  [  3.6802502 ,  -1.5486119 ,   3.407357  , ...,   8.523506  ,\n",
      "          -0.30161297,  -0.37603542]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8] _____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-5-27-64-4-8-32-True-0.0-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-True-0.0-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  7.273961  ,   2.6858864 ,   6.1284966 , ...,   3.488706  ,\n",
      "           1.8240454 ,   0.690683  ],\n",
      "        [ ...   [  4.0106564 ,   6.0699224 ,  -3.6050467 , ...,   2.3111353 ,\n",
      "          -2.659793  ,  -3.3891406 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-True-0.0-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c122081c0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[ 0.61286086,  1.503224  ,  1.3393495 , ...,  1.1939758 ,\n",
      "         -1.1025382 , -0.56685776],\n",
      "        [-1.0328...\n",
      "        [-1.3673017 ,  0.47134107, -0.25397953, ..., -0.20667079,\n",
      "          0.41074532, -1.7656472 ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12185430>, array([[[ 0.61286086,  1.503224  ,  1.3393495 , ...,  ...  [  4.0106564 ,   6.0699224 ,  -3.6050467 , ...,   2.3111353 ,\n",
      "          -2.659793  ,  -3.3891406 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 13.964778\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 131.63655\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 0.612861,  1.503224,  1.33935 , ...,  1.193976, -1.102538,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.566858],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.03289 ,  1.461785,  0.417585, ...,  0.165195, -1.335976,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  7.273961,   2.685886,   6.128497, ...,   3.488706,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                      1.824045,   0.690683],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  3.176486,   4.654918,   7.376852, ...,   5.429485,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12185430>, array([[[ 0.61286086,  1.503224  ,  1.3393495 , ...,  ...  [  4.0106564 ,   6.0699224 ,  -3.6050467 , ...,   2.3111353 ,\n",
      "          -2.659793  ,  -3.3891406 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-11-27-64-4-8-32-True-0.0-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-True-0.0-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  0.69647896,   0.19679128,   4.958303  , ...,   3.5340693 ,\n",
      "          12.056481  ,  -4.638954  ],\n",
      "        [ ...   [  1.9606065 ,  -6.0894413 ,   0.30727434, ...,   5.4533315 ,\n",
      "           0.4693191 ,   2.858992  ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-True-0.0-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121ca820>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[ 0.40893164, -0.03480246,  1.7259078 , ...,  2.5346494 ,\n",
      "          0.70684177, -0.849922  ],\n",
      "        [ 1.4188...\n",
      "        [-0.31656468,  0.9519379 ,  0.8777244 , ...,  0.4716997 ,\n",
      "         -0.02226987,  1.320401  ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121851f0>, array([[[ 0.40893164, -0.03480246,  1.7259078 , ...,  ...  [  1.9606065 ,  -6.0894413 ,   0.30727434, ...,   5.4533315 ,\n",
      "           0.4693191 ,   2.858992  ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 12.895727\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 206.19473\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 0.408932, -0.034802,  1.725908, ...,  2.534649,  0.706842,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.849922],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.418874,  0.924195,  0.831906, ...,  1.761815, -0.185403,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  0.696479,   0.196791,   4.958303, ...,   3.534069,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     12.056481,  -4.638954],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  3.958104,  -0.359839,   5.13902 , ...,   3.48922 ,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121851f0>, array([[[ 0.40893164, -0.03480246,  1.7259078 , ...,  ...  [  1.9606065 ,  -6.0894413 ,   0.30727434, ...,   5.4533315 ,\n",
      "           0.4693191 ,   2.858992  ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-5-27-64-2-8-32-False-0.1-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-False-0.1-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 0.6792545 , -4.4917946 ,  6.6487217 , ..., -1.4269075 ,\n",
      "         -5.737878  ,  0.2561761 ],\n",
      "        [-5.0192...\n",
      "        [-0.5947695 , -0.85105926,  4.6682377 , ..., -0.2126683 ,\n",
      "         -6.1225805 , -3.649379  ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-False-0.1-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121f5fa0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[ 5.0660926e-01, -8.9822567e-01,  5.9913802e-01, ...,\n",
      "          4.5206819e-02, -2.0716017e-01,  1.2421962e+00]...61e-01, -1.6623446e+00, -5.6830287e-01, ...,\n",
      "          8.8632172e-01,  8.0340904e-01, -1.7940395e+00]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12137040>, array([[[ 5.0660926e-01, -8.9822567e-01,  5.9913802e-0...        [-0.5947695 , -0.85105926,  4.6682377 , ..., -0.2126683 ,\n",
      "         -6.1225805 , -3.649379  ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 13.041257\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 173.55334\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 5.066093e-01, -8.982257e-01,  5.991380e-01, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     4.520682e-02, -2.071602e-01,  1.242196e+00],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 3.011488e-01, -7.365053e-01,  1.649249e+00, ...,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 0.679254, -4.491795,  6.648722, ..., -1.426908, -5.737878,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     0.256176],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-5.019225, -2.366311,  5.700693, ...,  1.109462, -5.428887,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12137040>, array([[[ 5.0660926e-01, -8.9822567e-01,  5.9913802e-0...        [-0.5947695 , -0.85105926,  4.6682377 , ..., -0.2126683 ,\n",
      "         -6.1225805 , -3.649379  ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-11-27-64-2-8-32-False-0.1-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-False-0.1-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 6.8925273e-01, -4.2810707e+00,  8.1032333e+00, ...,\n",
      "          3.6856699e+00,  5.9148903e+00, -6.6585503e+00]...01e+00, -2.3312631e+00,  2.4167006e+00, ...,\n",
      "          7.8214407e+00, -2.6203537e+00,  3.4568095e-01]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-False-0.1-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d19fd0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[ 1.0325667 , -1.1681921 ,  1.6835998 , ...,  1.3931553 ,\n",
      "         -0.5292031 , -1.7264102 ],\n",
      "        [-0.4210...\n",
      "        [-0.65434563,  1.322032  ,  0.6539206 , ...,  1.3419101 ,\n",
      "          1.1694114 ,  0.39871004]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12137700>, array([[[ 1.0325667 , -1.1681921 ,  1.6835998 , ...,  ...1e+00, -2.3312631e+00,  2.4167006e+00, ...,\n",
      "          7.8214407e+00, -2.6203537e+00,  3.4568095e-01]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 11.578186\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 654.9615\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 1.032567, -1.168192,  1.6836  , ...,  1.393155, -0.529203,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -1.72641 ],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.42105 ,  0.756308,  0.57341 , ...,  1.057016, -0.028595,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 6.892527e-01, -4.281071e+00,  8.103233e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     3.685670e+00,  5.914890e+00, -6.658550e+00],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 3.561299e-01, -2.217727e+00,  5.421297e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12137700>, array([[[ 1.0325667 , -1.1681921 ,  1.6835998 , ...,  ...1e+00, -2.3312631e+00,  2.4167006e+00, ...,\n",
      "          7.8214407e+00, -2.6203537e+00,  3.4568095e-01]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-5-27-64-4-8-32-False-0.1-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-False-0.1-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  4.139565  ,  -4.4098268 ,   6.0860357 , ...,   4.5159926 ,\n",
      "          -1.3411045 ,  -0.16460869],\n",
      "        [ ...   [  4.4545717 ,  -1.6818357 ,   1.5380583 , ...,   5.077571  ,\n",
      "          -2.86561   ,  -4.7460113 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-False-0.1-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d27a00>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[ 4.26044583e-01,  1.71181417e+00,  2.02984977e+00, ...,\n",
      "          1.51618421e-01, -8.17432225e-01, -6.0239064...41512668e+00,  2.89033562e-01, ...,\n",
      "         -9.01688814e-01,  1.58669382e-01, -1.56029999e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121379d0>, array([[[ 4.26044583e-01,  1.71181417e+00,  2.02984977...  [  4.4545717 ,  -1.6818357 ,   1.5380583 , ...,   5.077571  ,\n",
      "          -2.86561   ,  -4.7460113 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 17.105722\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 806.14746\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 4.260446e-01,  1.711814e+00,  2.029850e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     1.516184e-01, -8.174322e-01, -6.023906e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.473933e-01,  4.170398e-01,  1.936806e+00, ...,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  4.139565,  -4.409827,   6.086036, ...,   4.515993,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     -1.341105,  -0.164609],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  6.479433,   0.132786,   2.357935, ...,   5.31131 ,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121379d0>, array([[[ 4.26044583e-01,  1.71181417e+00,  2.02984977...  [  4.4545717 ,  -1.6818357 ,   1.5380583 , ...,   5.077571  ,\n",
      "          -2.86561   ,  -4.7460113 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-11-27-64-4-8-32-False-0.1-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-False-0.1-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ -3.578742  ,  -4.804233  ,   1.7275481 , ...,   3.7659585 ,\n",
      "           4.116438  ,  -3.0344179 ],\n",
      "        [ ...   [  0.9288589 ,  -6.0395317 ,  -0.7587563 , ...,   2.423737  ,\n",
      "           7.020883  ,  -3.4774485 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-False-0.1-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121e0610>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[-1.28086579e+00,  1.59733713e+00,  1.32976055e+00, ...,\n",
      "          7.65382469e-01, -5.66754222e-01,  2.1291860...85382521e-01,  1.05287623e+00, ...,\n",
      "          9.04305577e-02, -2.75667518e-01,  1.61204529e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12137ee0>, array([[[-1.28086579e+00,  1.59733713e+00,  1.32976055...  [  0.9288589 ,  -6.0395317 ,  -0.7587563 , ...,   2.423737  ,\n",
      "           7.020883  ,  -3.4774485 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 14.620227\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 1343.3483\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.280866e+00,  1.597337e+00,  1.329761e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     7.653825e-01, -5.667542e-01,  2.129186e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 2.478872e+00,  1.182258e+00,  1.050286e-01, ...,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ -3.578742,  -4.804233,   1.727548, ...,   3.765959,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                      4.116438,  -3.034418],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ -2.525174,  -6.666227,   4.401733, ...,  -0.024273,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12137ee0>, array([[[-1.28086579e+00,  1.59733713e+00,  1.32976055...  [  0.9288589 ,  -6.0395317 ,  -0.7587563 , ...,   2.423737  ,\n",
      "           7.020883  ,  -3.4774485 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8] _____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-5-27-64-2-8-32-True-0.1-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-True-0.1-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 3.5378537e+00, -1.0176097e+00,  2.5072663e+00, ...,\n",
      "         -1.2918103e-01,  1.9403224e+00, -2.0688248e-01]...17e+00,  1.5706275e+00,  3.0684795e+00, ...,\n",
      "         -7.4280578e-01, -8.7297440e+00, -4.7337089e+00]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-True-0.1-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c12186940>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[-1.1883738 ,  0.19796063,  0.8637698 , ...,  0.5472697 ,\n",
      "          0.16327298,  0.19552183],\n",
      "        [ 0.6456...\n",
      "        [ 0.57474184, -1.6039928 , -0.29848   , ...,  0.9193212 ,\n",
      "          0.6075021 , -1.7386186 ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12185310>, array([[[-1.1883738 ,  0.19796063,  0.8637698 , ...,  ...7e+00,  1.5706275e+00,  3.0684795e+00, ...,\n",
      "         -7.4280578e-01, -8.7297440e+00, -4.7337089e+00]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 16.699898\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 368.74008\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.188374,  0.197961,  0.86377 , ...,  0.54727 ,  0.163273,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     0.195522],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.645671,  0.092673,  1.105423, ...,  1.301436, -1.112109,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 3.537854e+00, -1.017610e+00,  2.507266e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -1.291810e-01,  1.940322e+00, -2.068825e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.069412e+01, -1.520096e+00,  4.908982e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12185310>, array([[[-1.1883738 ,  0.19796063,  0.8637698 , ...,  ...7e+00,  1.5706275e+00,  3.0684795e+00, ...,\n",
      "         -7.4280578e-01, -8.7297440e+00, -4.7337089e+00]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-11-27-64-2-8-32-True-0.1-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-True-0.1-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 1.80630434e+00, -3.29734874e+00,  2.05142379e+00, ...,\n",
      "          5.15636086e-01,  1.09334707e+01, -5.3997001...86660719e+00,  1.96528840e+00, ...,\n",
      "          8.17727089e+00, -3.29322815e+00,  6.09638810e-01]]],\n",
      "      dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-True-0.1-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d401f0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[-1.7457623 ,  0.48317087,  0.7995027 , ...,  0.6881164 ,\n",
      "         -0.04270777, -0.43860584],\n",
      "        [-0.7674...\n",
      "        [-0.47491103,  1.0178024 ,  0.22599775, ...,  1.2424492 ,\n",
      "          0.748508  ,  0.64233625]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121ce790>, array([[[-1.7457623 ,  0.48317087,  0.7995027 , ...,  ...6660719e+00,  1.96528840e+00, ...,\n",
      "          8.17727089e+00, -3.29322815e+00,  6.09638810e-01]]],\n",
      "      dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 13.072953\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 293.5055\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.745762,  0.483171,  0.799503, ...,  0.688116, -0.042708,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.438606],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.767437,  0.970401, -0.542354, ...,  0.268911, -0.399521,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 1.806304e+00, -3.297349e+00,  2.051424e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     5.156361e-01,  1.093347e+01, -5.399700e+00],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 3.403076e+00, -4.039076e+00,  2.029054e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121ce790>, array([[[-1.7457623 ,  0.48317087,  0.7995027 , ...,  ...6660719e+00,  1.96528840e+00, ...,\n",
      "          8.17727089e+00, -3.29322815e+00,  6.09638810e-01]]],\n",
      "      dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8] _____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-5-27-64-4-8-32-True-0.1-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-True-0.1-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 11.824695  ,   0.56448996,   8.893088  , ...,   5.3888826 ,\n",
      "          -0.63698864,  -0.8577647 ],\n",
      "        [ ...   [  4.014583  ,   0.1092521 ,   0.57215273, ...,   2.7110734 ,\n",
      "          -5.671545  ,  -3.9170866 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-True-0.1-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d30970>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[-0.24720213,  2.1872036 ,  1.1780909 , ...,  1.0841107 ,\n",
      "         -1.1772475 , -0.9516682 ],\n",
      "        [-1.7174...\n",
      "        [-0.61196035,  1.503826  ,  0.20703173, ..., -1.4228965 ,\n",
      "          0.5199669 , -1.4305847 ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12185e50>, array([[[-0.24720213,  2.1872036 ,  1.1780909 , ...,  ...  [  4.014583  ,   0.1092521 ,   0.57215273, ...,   2.7110734 ,\n",
      "          -5.671545  ,  -3.9170866 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 15.75727\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 46.330788\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.247202,  2.187204,  1.178091, ...,  1.084111, -1.177248,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.951668],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.717414,  1.954637,  0.480065, ..., -0.309407, -1.426775,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 11.824695,   0.56449 ,   8.893088, ...,   5.388883,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     -0.636989,  -0.857765],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  9.349314,   2.890456,   6.584941, ...,   4.397396,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12185e50>, array([[[-0.24720213,  2.1872036 ,  1.1780909 , ...,  ...  [  4.014583  ,   0.1092521 ,   0.57215273, ...,   2.7110734 ,\n",
      "          -5.671545  ,  -3.9170866 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-11-27-64-4-8-32-True-0.1-cpu()'\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-True-0.1-cpu().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  4.2638364 ,   1.4296489 ,   8.03221   , ...,   6.577572  ,\n",
      "           4.3914165 ,   1.4488723 ],\n",
      "        [ ...   [  1.502727  ,  -6.356389  ,  -2.0409884 , ...,   3.0519426 ,\n",
      "           4.5898957 ,   0.7807688 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-True-0.1-cpu().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121d8a60>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[-0.12271383, -0.7592321 ,  1.4124345 , ...,  1.9677969 ,\n",
      "          1.6120886 , -0.7598106 ],\n",
      "        [ 1.5310...\n",
      "        [-0.01644097,  1.4803139 ,  0.25631025, ...,  0.24426797,\n",
      "         -0.49234763,  0.93699133]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121854c0>, array([[[-0.12271383, -0.7592321 ,  1.4124345 , ...,  ...  [  1.502727  ,  -6.356389  ,  -2.0409884 , ...,   3.0519426 ,\n",
      "           4.5898957 ,   0.7807688 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 16.932209\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 291.311\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.122714, -0.759232,  1.412434, ...,  1.967797,  1.612089,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.759811],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.531079,  1.467547,  1.007208, ...,  1.484929,  0.723991,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  4.263836,   1.429649,   8.03221 , ...,   6.577572,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                      4.391417,   1.448872],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  2.934974,   2.086097,   0.94253 , ...,   5.618777,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121854c0>, array([[[-0.12271383, -0.7592321 ,  1.4124345 , ...,  ...  [  1.502727  ,  -6.356389  ,  -2.0409884 , ...,   3.0519426 ,\n",
      "           4.5898957 ,   0.7807688 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-5-27-64-2-8-32-False-0.0-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-False-0.0-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[-5.5949032e-01, -2.1942677e+00,  6.1424117e+00, ...,\n",
      "          2.4950914e+00, -5.0963488e+00,  9.5644641e-01]...32e+00,  1.4364984e+00,  3.4052649e+00, ...,\n",
      "          1.9017427e+00, -4.3343668e+00, -1.5436333e+00]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-False-0.0-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121edb20>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[ 1.27858448e+00, -4.19090509e-01,  8.84249032e-01, ...,\n",
      "          2.01707914e-01, -2.82128036e-01,  6.5203887...75588238e+00,  1.69019684e-01, ...,\n",
      "          1.03529704e+00,  5.99163592e-01, -1.33877468e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12137940>, array([[[ 1.27858448e+00, -4.19090509e-01,  8.84249032...2e+00,  1.4364984e+00,  3.4052649e+00, ...,\n",
      "          1.9017427e+00, -4.3343668e+00, -1.5436333e+00]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 11.499633\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 1325.5636\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 1.278584e+00, -4.190905e-01,  8.842490e-01, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     2.017079e-01, -2.821280e-01,  6.520389e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.149984e+00, -2.646869e-01,  1.720497e+00, ...,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[-5.594903e-01, -2.194268e+00,  6.142412e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     2.495091e+00, -5.096349e+00,  9.564464e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-2.803166e+00, -3.013344e+00,  3.758646e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12137940>, array([[[ 1.27858448e+00, -4.19090509e-01,  8.84249032...2e+00,  1.4364984e+00,  3.4052649e+00, ...,\n",
      "          1.9017427e+00, -4.3343668e+00, -1.5436333e+00]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8] ___________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-11-27-64-2-8-32-False-0.0-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-False-0.0-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[-1.41697085e+00, -3.12597418e+00,  6.57922649e+00, ...,\n",
      "          8.14910603e+00,  2.10118008e+00, -6.5926976...42078924e+00,  3.61126113e+00, ...,\n",
      "          7.04340839e+00, -7.76696920e-01, -1.25795877e+00]]],\n",
      "      dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-False-0.0-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d0e8b0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[ 0.62911695, -0.5162204 ,  1.0636867 , ..., -0.12832814,\n",
      "         -1.0119822 , -2.2170863 ],\n",
      "        [-0.3328...\n",
      "        [-0.3270652 ,  1.967925  ,  0.3595866 , ...,  0.19388634,\n",
      "          1.3994881 ,  0.6569602 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121375e0>, array([[[ 0.62911695, -0.5162204 ,  1.0636867 , ..., -...2078924e+00,  3.61126113e+00, ...,\n",
      "          7.04340839e+00, -7.76696920e-01, -1.25795877e+00]]],\n",
      "      dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 10.5228815\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 1600.1642\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 0.629117, -0.51622 ,  1.063687, ..., -0.128328, -1.011982,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -2.217086],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.332841,  1.152816,  0.949509, ...,  0.775403, -0.032665,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[-1.416971e+00, -3.125974e+00,  6.579226e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     8.149106e+00,  2.101180e+00, -6.592698e+00],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.830267e+00, -7.977772e-01,  8.756701e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121375e0>, array([[[ 0.62911695, -0.5162204 ,  1.0636867 , ..., -...2078924e+00,  3.61126113e+00, ...,\n",
      "          7.04340839e+00, -7.76696920e-01, -1.25795877e+00]]],\n",
      "      dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-5-27-64-4-8-32-False-0.0-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-False-0.0-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 3.659358  ,  0.6878738 ,  2.3118215 , ...,  3.2671478 ,\n",
      "         -2.7092302 ,  4.8205247 ],\n",
      "        [ 0.5323...\n",
      "        [ 6.4801354 ,  1.5321803 , -0.2689283 , ...,  4.1558657 ,\n",
      "          0.5391748 , -3.4294167 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-False-0.0-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121b48e0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[ 1.1966805 ,  1.4044719 ,  1.0587275 , ...,  0.8214377 ,\n",
      "         -1.55321   , -0.39313436],\n",
      "        [ 0.6073...\n",
      "        [-1.4070045 ,  1.0843719 ,  0.4441776 , ...,  0.22686525,\n",
      "          0.50010926, -1.992602  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12137820>, array([[[ 1.1966805 ,  1.4044719 ,  1.0587275 , ...,  ...        [ 6.4801354 ,  1.5321803 , -0.2689283 , ...,  4.1558657 ,\n",
      "          0.5391748 , -3.4294167 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 16.29073\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 86.167786\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 1.196681,  1.404472,  1.058728, ...,  0.821438, -1.55321 ,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.393134],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.607399,  0.888673,  2.190732, ...,  0.690429, -1.704219,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 3.659358,  0.687874,  2.311821, ...,  3.267148, -2.70923 ,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     4.820525],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.532355, -2.781025,  0.516811, ...,  4.018682, -6.554437,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12137820>, array([[[ 1.1966805 ,  1.4044719 ,  1.0587275 , ...,  ...        [ 6.4801354 ,  1.5321803 , -0.2689283 , ...,  4.1558657 ,\n",
      "          0.5391748 , -3.4294167 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8] ___________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-11-27-64-4-8-32-False-0.0-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-False-0.0-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[-2.6986742 , -3.59624   ,  2.5475888 , ...,  8.575455  ,\n",
      "          5.4873843 , -0.9009904 ],\n",
      "        [-2.6018...\n",
      "        [ 1.9172053 , -5.8891563 ,  2.418453  , ...,  5.9232807 ,\n",
      "          2.3124108 ,  0.9898863 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-False-0.0-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d2cb50>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[-0.42570964,  1.7394766 ,  0.8601767 , ...,  1.1444634 ,\n",
      "         -0.7816325 ,  0.52424455],\n",
      "        [ 2.2220...\n",
      "        [-0.4602054 ,  0.774147  ,  0.7113594 , ...,  0.15940991,\n",
      "         -0.07540286,  1.907063  ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c120cb4c0>, array([[[-0.42570964,  1.7394766 ,  0.8601767 , ...,  ...        [ 1.9172053 , -5.8891563 ,  2.418453  , ...,  5.9232807 ,\n",
      "          2.3124108 ,  0.9898863 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 13.273767\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 262.36923\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.42571 ,  1.739477,  0.860177, ...,  1.144463, -0.781632,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     0.524245],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 2.222084,  0.400268,  0.166152, ...,  0.590903, -1.03512 ,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[-2.698674, -3.59624 ,  2.547589, ...,  8.575455,  5.487384,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.90099 ],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-2.601815, -6.391165,  7.881776, ...,  2.094327, -2.067451,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c120cb4c0>, array([[[-0.42570964,  1.7394766 ,  0.8601767 , ...,  ...        [ 1.9172053 , -5.8891563 ,  2.418453  , ...,  5.9232807 ,\n",
      "          2.3124108 ,  0.9898863 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-5-27-64-2-8-32-True-0.0-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-True-0.0-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  1.6476443 ,  -1.8613685 ,   3.260939  , ...,   1.4043771 ,\n",
      "          -1.7647722 ,   0.53275806],\n",
      "        [ ...   [ -2.4991956 ,   2.2171898 ,   1.1389779 , ...,   0.9479268 ,\n",
      "          -5.7012367 ,  -1.991215  ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-True-0.0-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d64df0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[-0.5885171 ,  0.59811425,  0.8724936 , ...,  0.51767856,\n",
      "          0.13866618, -0.06546182],\n",
      "        [ 0.3603...\n",
      "        [ 0.992541  , -1.6418681 ,  0.27018586, ...,  1.0907836 ,\n",
      "          0.36180976, -1.238305  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c120cb820>, array([[[-0.5885171 ,  0.59811425,  0.8724936 , ...,  ...  [ -2.4991956 ,   2.2171898 ,   1.1389779 , ...,   0.9479268 ,\n",
      "          -5.7012367 ,  -1.991215  ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 12.618837\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 377.02283\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.588517,  0.598114,  0.872494, ...,  0.517679,  0.138666,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.065462],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.360305, -0.377494,  0.790841, ...,  1.554729, -0.574109,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  1.647644,  -1.861369,   3.260939, ...,   1.404377,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     -1.764772,   0.532758],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ -4.428632,   0.331761,   3.801016, ...,   2.275623,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c120cb820>, array([[[-0.5885171 ,  0.59811425,  0.8724936 , ...,  ...  [ -2.4991956 ,   2.2171898 ,   1.1389779 , ...,   0.9479268 ,\n",
      "          -5.7012367 ,  -1.991215  ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-11-27-64-2-8-32-True-0.0-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-True-0.0-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  2.6538882 ,  -2.3525443 ,   3.5054455 , ...,   1.8293396 ,\n",
      "          10.294293  ,  -4.634384  ],\n",
      "        [ ...   [  3.680252  ,  -1.5486114 ,   3.4073567 , ...,   8.523508  ,\n",
      "          -0.3016137 ,  -0.37603462]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-True-0.0-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121296a0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[-1.3267026 ,  0.17535046,  0.5994783 , ...,  0.20292987,\n",
      "         -0.35895693, -0.17188787],\n",
      "        [-0.7986...\n",
      "        [-0.47717363,  1.6211251 , -0.09050198, ..., -0.11199577,\n",
      "          0.98024064,  0.9043471 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121ce9d0>, array([[[-1.3267026 ,  0.17535046,  0.5994783 , ...,  ...  [  3.680252  ,  -1.5486114 ,   3.4073567 , ...,   8.523508  ,\n",
      "          -0.3016137 ,  -0.37603462]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 12.03532\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 265.11017\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.326703,  0.17535 ,  0.599478, ...,  0.20293 , -0.358957,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.171888],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.798601,  0.050448, -0.81543 , ...,  0.352967, -0.525187,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  2.653888,  -2.352544,   3.505445, ...,   1.82934 ,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     10.294293,  -4.634384],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  2.769305,  -2.337574,   3.823559, ...,   2.624812,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121ce9d0>, array([[[-1.3267026 ,  0.17535046,  0.5994783 , ...,  ...  [  3.680252  ,  -1.5486114 ,   3.4073567 , ...,   8.523508  ,\n",
      "          -0.3016137 ,  -0.37603462]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-5-27-64-4-8-32-True-0.0-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-True-0.0-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  7.2739635 ,   2.685885  ,   6.128495  , ...,   3.4887066 ,\n",
      "           1.8240458 ,   0.6906817 ],\n",
      "        [ ...   [  4.010655  ,   6.069924  ,  -3.605043  , ...,   2.311139  ,\n",
      "          -2.6597958 ,  -3.3891416 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-True-0.0-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d839d0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[ 0.6128605 ,  1.5032239 ,  1.3393492 , ...,  1.1939758 ,\n",
      "         -1.1025381 , -0.5668576 ],\n",
      "        [-1.0328...\n",
      "        [-1.3673013 ,  0.47134113, -0.25397962, ..., -0.20667058,\n",
      "          0.41074634, -1.7656468 ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121cef70>, array([[[ 0.6128605 ,  1.5032239 ,  1.3393492 , ...,  ...  [  4.010655  ,   6.069924  ,  -3.605043  , ...,   2.311139  ,\n",
      "          -2.6597958 ,  -3.3891416 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 13.964779\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 131.67902\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 0.612861,  1.503224,  1.339349, ...,  1.193976, -1.102538,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.566858],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.03289 ,  1.461785,  0.417584, ...,  0.165195, -1.335976,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  7.273963,   2.685885,   6.128495, ...,   3.488707,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                      1.824046,   0.690682],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  3.176488,   4.654915,   7.376852, ...,   5.429483,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121cef70>, array([[[ 0.6128605 ,  1.5032239 ,  1.3393492 , ...,  ...  [  4.010655  ,   6.069924  ,  -3.605043  , ...,   2.311139  ,\n",
      "          -2.6597958 ,  -3.3891416 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-11-27-64-4-8-32-True-0.0-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-True-0.0-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  0.69648015,   0.19679132,   4.9583054 , ...,   3.5340679 ,\n",
      "          12.056481  ,  -4.638955  ],\n",
      "        [ ...   [  1.9606097 ,  -6.0894423 ,   0.30727303, ...,   5.453332  ,\n",
      "           0.469316  ,   2.8589888 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-True-0.0-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121eaf10>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[ 0.4089326 , -0.0348022 ,  1.725908  , ...,  2.534649  ,\n",
      "          0.7068406 , -0.8499228 ],\n",
      "        [ 1.4188...\n",
      "        [-0.31656438,  0.9519381 ,  0.8777248 , ...,  0.47170007,\n",
      "         -0.02226995,  1.3204006 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121379d0>, array([[[ 0.4089326 , -0.0348022 ,  1.725908  , ...,  ...  [  1.9606097 ,  -6.0894423 ,   0.30727303, ...,   5.453332  ,\n",
      "           0.469316  ,   2.8589888 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 12.895725\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 205.9597\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 0.408933, -0.034802,  1.725908, ...,  2.534649,  0.706841,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.849923],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.418874,  0.924195,  0.831906, ...,  1.761815, -0.185403,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  0.69648 ,   0.196791,   4.958305, ...,   3.534068,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     12.056481,  -4.638955],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  3.958104,  -0.359836,   5.139021, ...,   3.489223,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121379d0>, array([[[ 0.4089326 , -0.0348022 ,  1.725908  , ...,  ...  [  1.9606097 ,  -6.0894423 ,   0.30727303, ...,   5.453332  ,\n",
      "           0.469316  ,   2.8589888 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-5-27-64-2-8-32-False-0.1-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-False-0.1-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 0.67925584, -4.4917946 ,  6.6487217 , ..., -1.4269089 ,\n",
      "         -5.737878  ,  0.25617456],\n",
      "        [-5.0192...\n",
      "        [-0.59476876, -0.8510582 ,  4.6682363 , ..., -0.21266806,\n",
      "         -6.1225796 , -3.6493788 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-False-0.1-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121f8d60>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[ 5.06609380e-01, -8.98226321e-01,  5.99137723e-01, ...,\n",
      "          4.52070236e-02, -2.07160711e-01,  1.2421962...66234422e+00, -5.68302870e-01, ...,\n",
      "          8.86321723e-01,  8.03409040e-01, -1.79403937e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12137ca0>, array([[[ 5.06609380e-01, -8.98226321e-01,  5.99137723...        [-0.59476876, -0.8510582 ,  4.6682363 , ..., -0.21266806,\n",
      "         -6.1225796 , -3.6493788 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 13.04126\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 173.55878\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 5.066094e-01, -8.982263e-01,  5.991377e-01, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     4.520702e-02, -2.071607e-01,  1.242196e+00],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 3.011490e-01, -7.365055e-01,  1.649249e+00, ...,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 0.679256, -4.491795,  6.648722, ..., -1.426909, -5.737878,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     0.256175],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-5.019225, -2.366312,  5.70069 , ...,  1.109461, -5.428886,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12137ca0>, array([[[ 5.06609380e-01, -8.98226321e-01,  5.99137723...        [-0.59476876, -0.8510582 ,  4.6682363 , ..., -0.21266806,\n",
      "         -6.1225796 , -3.6493788 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8] ___________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-11-27-64-2-8-32-False-0.1-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-False-0.1-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 6.8925166e-01, -4.2810693e+00,  8.1032352e+00, ...,\n",
      "          3.6856694e+00,  5.9148889e+00, -6.6585488e+00]...75e+00, -2.3312619e+00,  2.4167008e+00, ...,\n",
      "          7.8214397e+00, -2.6203525e+00,  3.4568369e-01]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-False-0.1-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d29700>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[ 1.032567  , -1.1681925 ,  1.6836002 , ...,  1.3931551 ,\n",
      "         -0.5292028 , -1.7264104 ],\n",
      "        [-0.4210...\n",
      "        [-0.6543457 ,  1.3220321 ,  0.6539206 , ...,  1.3419099 ,\n",
      "          1.1694117 ,  0.3987097 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c120cb430>, array([[[ 1.032567  , -1.1681925 ,  1.6836002 , ...,  ...5e+00, -2.3312619e+00,  2.4167008e+00, ...,\n",
      "          7.8214397e+00, -2.6203525e+00,  3.4568369e-01]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 11.578188\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 655.8394\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 1.032567, -1.168193,  1.6836  , ...,  1.393155, -0.529203,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -1.72641 ],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.421051,  0.756308,  0.57341 , ...,  1.057017, -0.028596,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 6.892517e-01, -4.281069e+00,  8.103235e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     3.685669e+00,  5.914889e+00, -6.658549e+00],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 3.561307e-01, -2.217727e+00,  5.421297e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c120cb430>, array([[[ 1.032567  , -1.1681925 ,  1.6836002 , ...,  ...5e+00, -2.3312619e+00,  2.4167008e+00, ...,\n",
      "          7.8214397e+00, -2.6203525e+00,  3.4568369e-01]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-5-27-64-4-8-32-False-0.1-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-False-0.1-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  4.1395674 ,  -4.4098268 ,   6.0860357 , ...,   4.5159955 ,\n",
      "          -1.3411058 ,  -0.1646119 ],\n",
      "        [ ...   [  4.4545755 ,  -1.6818367 ,   1.5380597 , ...,   5.077572  ,\n",
      "          -2.8656082 ,  -4.7460103 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-False-0.1-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d1b910>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[ 4.2604440e-01,  1.7118144e+00,  2.0298505e+00, ...,\n",
      "          1.5161809e-01, -8.1743234e-01, -6.0239083e-01]...47e+00,  1.4151264e+00,  2.8903374e-01, ...,\n",
      "         -9.0168846e-01,  1.5866904e-01, -1.5602999e+00]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c120cbca0>, array([[[ 4.2604440e-01,  1.7118144e+00,  2.0298505e+0...  [  4.4545755 ,  -1.6818367 ,   1.5380597 , ...,   5.077572  ,\n",
      "          -2.8656082 ,  -4.7460103 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 17.105722\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 803.05115\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[ 4.260444e-01,  1.711814e+00,  2.029850e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     1.516181e-01, -8.174323e-01, -6.023908e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.473928e-01,  4.170405e-01,  1.936808e+00, ...,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  4.139567,  -4.409827,   6.086036, ...,   4.515996,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     -1.341106,  -0.164612],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  6.479443,   0.132787,   2.357936, ...,   5.311312,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c120cbca0>, array([[[ 4.2604440e-01,  1.7118144e+00,  2.0298505e+0...  [  4.4545755 ,  -1.6818367 ,   1.5380597 , ...,   5.077572  ,\n",
      "          -2.8656082 ,  -4.7460103 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8] ___________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "current_input_id = '8-11-27-64-4-8-32-False-0.1-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-False-0.1-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ -3.578744  ,  -4.8042345 ,   1.7275448 , ...,   3.7659588 ,\n",
      "           4.1164355 ,  -3.0344205 ],\n",
      "        [ ...   [  0.9288554 ,  -6.03953   ,  -0.75875497, ...,   2.4237394 ,\n",
      "           7.020886  ,  -3.4774513 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-False-0.1-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d251f0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[-1.28086627e+00,  1.59733689e+00,  1.32976055e+00, ...,\n",
      "          7.65381932e-01, -5.66753864e-01,  2.1291887...85382760e-01,  1.05287600e+00, ...,\n",
      "          9.04308781e-02, -2.75667012e-01,  1.61204493e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c120f7670>, array([[[-1.28086627e+00,  1.59733689e+00,  1.32976055...  [  0.9288554 ,  -6.03953   ,  -0.75875497, ...,   2.4237394 ,\n",
      "           7.020886  ,  -3.4774513 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 14.620229\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 1341.8912\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.280866e+00,  1.597337e+00,  1.329761e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     7.653819e-01, -5.667539e-01,  2.129189e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 2.478872e+00,  1.182257e+00,  1.050289e-01, ...,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ -3.578744,  -4.804235,   1.727545, ...,   3.765959,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                      4.116436,  -3.03442 ],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ -2.525173,  -6.666224,   4.401733, ...,  -0.024275,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c120f7670>, array([[[-1.28086627e+00,  1.59733689e+00,  1.32976055...  [  0.9288554 ,  -6.03953   ,  -0.75875497, ...,   2.4237394 ,\n",
      "           7.020886  ,  -3.4774513 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-5-27-64-2-8-32-True-0.1-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-True-0.1-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 3.5378535e+00, -1.0176097e+00,  2.5072670e+00, ...,\n",
      "         -1.2918162e-01,  1.9403243e+00, -2.0688322e-01]...03e+00,  1.5706289e+00,  3.0684781e+00, ...,\n",
      "         -7.4280375e-01, -8.7297440e+00, -4.7337103e+00]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-2-8-32-True-0.1-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0ac2d30220>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[-1.1883738 ,  0.19796063,  0.8637698 , ...,  0.5472697 ,\n",
      "          0.16327298,  0.19552183],\n",
      "        [ 0.6456...\n",
      "        [ 0.57474196, -1.6039932 , -0.29847988, ...,  0.9193213 ,\n",
      "          0.60750204, -1.7386185 ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c120f7310>, array([[[-1.1883738 ,  0.19796063,  0.8637698 , ...,  ...3e+00,  1.5706289e+00,  3.0684781e+00, ...,\n",
      "         -7.4280375e-01, -8.7297440e+00, -4.7337103e+00]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 16.699896\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 368.688\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.188374,  0.197961,  0.86377 , ...,  0.54727 ,  0.163273,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     0.195522],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.645671,  0.092674,  1.105423, ...,  1.301436, -1.112109,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 3.537853e+00, -1.017610e+00,  2.507267e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -1.291816e-01,  1.940324e+00, -2.068832e-01],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.069411e+01, -1.520096e+00,  4.908981e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c120f7310>, array([[[-1.1883738 ,  0.19796063,  0.8637698 , ...,  ...3e+00,  1.5706289e+00,  3.0684781e+00, ...,\n",
      "         -7.4280375e-01, -8.7297440e+00, -4.7337103e+00]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-11-27-64-2-8-32-True-0.1-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-True-0.1-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 1.8063045e+00, -3.2973490e+00,  2.0514235e+00, ...,\n",
      "          5.1563561e-01,  1.0933471e+01, -5.3997021e+00]...48e+00, -1.8666056e+00,  1.9652903e+00, ...,\n",
      "          8.1772728e+00, -3.2932296e+00,  6.0964048e-01]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-2-8-32-True-0.1-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c120fb880>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "result     = array([[[-1.7457622 ,  0.48317087,  0.79950273, ...,  0.6881165 ,\n",
      "         -0.04270759, -0.43860576],\n",
      "        [-0.7674...\n",
      "        [-0.474911  ,  1.0178024 ,  0.22599764, ...,  1.2424494 ,\n",
      "          0.74850816,  0.64233583]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c120f7c10>, array([[[-1.7457622 ,  0.48317087,  0.79950273, ...,  ...8e+00, -1.8666056e+00,  1.9652903e+00, ...,\n",
      "          8.1772728e+00, -3.2932296e+00,  6.0964048e-01]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 13.072953\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 293.51804\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.745762,  0.483171,  0.799503, ...,  0.688116, -0.042708,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.438606],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.767437,  0.9704  , -0.542355, ...,  0.268911, -0.399522,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 1.806304e+00, -3.297349e+00,  2.051424e+00, ...,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     5.156356e-01,  1.093347e+01, -5.399702e+00],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 3.403075e+00, -4.039074e+00,  2.029055e+00, ...,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c120f7c10>, array([[[-1.7457622 ,  0.48317087,  0.79950273, ...,  ...8e+00, -1.8666056e+00,  1.9652903e+00, ...,\n",
      "          8.1772728e+00, -3.2932296e+00,  6.0964048e-01]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "..... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-5-27-64-4-8-32-True-0.1-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-True-0.1-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[ 11.824696  ,   0.56449115,   8.893089  , ...,   5.3888807 ,\n",
      "          -0.63698757,  -0.8577647 ],\n",
      "        [ ...   [  4.014581  ,   0.10925543,   0.57214814, ...,   2.7110748 ,\n",
      "          -5.671551  ,  -3.9170878 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-5-27-64-4-8-32-True-0.1-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c121c9a60>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[-0.24720213,  2.1872036 ,  1.1780909 , ...,  1.0841107 ,\n",
      "         -1.1772475 , -0.9516682 ],\n",
      "        [-1.7174...\n",
      "        [-0.61196053,  1.5038253 ,  0.20703137, ..., -1.4228958 ,\n",
      "          0.5199665 , -1.4305837 ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c12137ca0>, array([[[-0.24720213,  2.1872036 ,  1.1780909 , ...,  ...  [  4.014581  ,   0.10925543,   0.57214814, ...,   2.7110748 ,\n",
      "          -5.671551  ,  -3.9170878 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1080 / 1080 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 15.757275\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 46.32983\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.247202,  2.187204,  1.178091, ...,  1.084111, -1.177248,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.951668],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.717414,  1.954636,  0.480065, ..., -0.309408, -1.426776,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[ 11.824696,   0.564491,   8.893089, ...,   5.388881,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     -0.636988,  -0.857765],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  9.349315,   2.890455,   6.584943, ...,   4.397395,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c12137ca0>, array([[[-0.24720213,  2.1872036 ,  1.1780909 , ...,  ...  [  4.014581  ,   0.10925543,   0.57214814, ...,   2.7110748 ,\n",
      "          -5.671551  ,  -3.9170878 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result, _ = model(ndl_x)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = result.numpy()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        current_input_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([\u001b[96mstr\u001b[39;49;00m(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device\u001b[90m\u001b[39;49;00m\n",
      "        )])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        labels_path = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./tests/hw4_extra/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_transformer_model-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.npy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            .format(current_input_id))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(labels_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m f:\u001b[90m\u001b[39;49;00m\n",
      "            label_result = np.load(f)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(result, label_result, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "_          = needle.Tensor([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0...0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]])\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "current_input_id = '8-11-27-64-4-8-32-True-0.1-cuda()'\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "f          = <_io.BufferedReader name='./tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-True-0.1-cuda().npy'>\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "label_result = array([[[  4.263836  ,   1.4296501 ,   8.032211  , ...,   6.577574  ,\n",
      "           4.391417  ,   1.4488727 ],\n",
      "        [ ...   [  1.5027263 ,  -6.3563886 ,  -2.0409875 , ...,   3.0519385 ,\n",
      "           4.589894  ,   0.7807694 ]]], dtype=float32)\n",
      "labels_path = './tests/hw4_extra/data/test_transformer_model-8-11-27-64-4-8-32-True-0.1-cuda().npy'\n",
      "model      = <needle.nn.nn_transformer.Transformer object at 0x7f0c1210ccd0>\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "result     = array([[[-0.12271412, -0.7592318 ,  1.412434  , ...,  1.9677963 ,\n",
      "          1.6120886 , -0.75981057],\n",
      "        [ 1.5310...\n",
      "        [-0.01644098,  1.4803138 ,  0.25631148, ...,  0.24426809,\n",
      "         -0.49234822,  0.9369918 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:213: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f0c121378b0>, array([[[-0.12271412, -0.7592318 ,  1.412434  , ...,  ...  [  1.5027263 ,  -6.3563886 ,  -2.0409875 , ...,   3.0519385 ,\n",
      "           4.589894  ,   0.7807694 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 2376 / 2376 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference among violations: 16.932209\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference among violations: 291.15985\u001b[0m\n",
      "\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.122714, -0.759232,  1.412434, ...,  1.967796,  1.612089,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                    -0.759811],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.531079,  1.467546,  1.007208, ...,  1.484928,  0.723991,...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            DESIRED: array([[[  4.263836,   1.42965 ,   8.032211, ...,   6.577574,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                      4.391417,   1.448873],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [  2.934974,   2.086098,   0.94253 , ...,   5.618779,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f0c121378b0>, array([[[-0.12271412, -0.7592318 ,  1.412434  , ...,  ...  [  1.5027263 ,  -6.3563886 ,  -2.0409875 , ...,   3.0519385 ,\n",
      "           4.589894  ,   0.7807694 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f0ac2ced0d0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f0ac2d11f40>\n",
      "\n",
      "\u001b[1m\u001b[31m/root/miniconda3/envs/LightTorch/lib/python3.9/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8]\u001b[0m - AssertionError: \n",
      "\u001b[31m====================== \u001b[31m\u001b[1m32 failed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 4.13s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"transformer_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c897377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR_KEY_HERE\" -k \"transformer_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899683fc",
   "metadata": {},
   "source": [
    "Now, you can train a Transformer language model on the Penn Treebank dataset:\n",
    "\n",
    "Note: make sure to initialize a transformer model in the class `LanguageModel` of `apps/models.py`; also for Transformers, the final linear head `self.linear` should take in input dimension `embedding_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d118e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using needle backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [13:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_acc: 0.04345477862700544, avg_loss: 7.289129734039307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [12:54<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_acc: 0.05107200647249191, avg_loss: 6.596351623535156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [12:54<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_acc: 0.05193916546168147, avg_loss: 6.58458137512207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [12:52<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_acc: 0.051618553329201955, avg_loss: 6.578758716583252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [12:53<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_acc: 0.05231249569648144, avg_loss: 6.57459831237793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [12:55<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_acc: 0.05308067375886525, avg_loss: 6.567375659942627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [12:52<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_acc: 0.05300321042484335, avg_loss: 6.559388637542725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [14:41<00:00,  1.21s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_acc: 0.052470650003442816, avg_loss: 6.5526275634765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 336/727 [05:55<06:53,  1.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m train_data \u001b[38;5;241m=\u001b[39m ndl\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mbatchify(corpus\u001b[38;5;241m.\u001b[39mtrain, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;28mlen\u001b[39m(corpus\u001b[38;5;241m.\u001b[39mdictionary), hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, seq_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_ptb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m evaluate_ptb(model, train_data, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/workspace/LightTorch/archive/hw4_extra/./apps/simple_ml.py:288\u001b[0m, in \u001b[0;36mtrain_ptb\u001b[0;34m(model, data, seq_len, n_epochs, optimizer, lr, weight_decay, loss_fn, clip, device, dtype)\u001b[0m\n\u001b[1;32m    285\u001b[0m opt \u001b[38;5;241m=\u001b[39m optimizer(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m lr, weight_decay \u001b[38;5;241m=\u001b[39m weight_decay)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 288\u001b[0m     avg_acc, avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mepoch_general_ptb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_acc, avg_loss\n",
      "File \u001b[0;32m~/workspace/LightTorch/archive/hw4_extra/./apps/simple_ml.py:234\u001b[0m, in \u001b[0;36mepoch_general_ptb\u001b[0;34m(data, model, seq_len, loss_fn, opt, clip, device, dtype)\u001b[0m\n\u001b[1;32m    232\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    233\u001b[0m dataset_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m--> 234\u001b[0m y_pred, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# detach the hidden state to avoid blowing up computational graph\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# between training on different sequences\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hidden, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/workspace/LightTorch/archive/hw4_extra/./python/needle/nn/nn_basic.py:76\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/LightTorch/archive/hw4_extra/./apps/models.py:91\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03mGiven sequence (and the previous hidden state if given), returns probabilities of next word\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m(along with the last hidden state from the sequence model).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    else h is tuple of (h0, c0), each of shape (num_layers, bs, hidden_size)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m### BEGIN YOUR SOLUTION\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m x, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_model(x, h)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_model, nn\u001b[38;5;241m.\u001b[39mRNN) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_model, nn\u001b[38;5;241m.\u001b[39mLSTM):\n",
      "File \u001b[0;32m~/workspace/LightTorch/archive/hw4_extra/./python/needle/nn/nn_basic.py:76\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/LightTorch/archive/hw4_extra/./python/needle/nn/nn_sequence.py:329\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m### BEGIN YOUR SOLUTION\u001b[39;00m\n\u001b[1;32m    328\u001b[0m seq_len, bs \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 329\u001b[0m x_one_hot \u001b[38;5;241m=\u001b[39m \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (x_one_hot\u001b[38;5;241m.\u001b[39mreshape((seq_len\u001b[38;5;241m*\u001b[39mbs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings)) \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\u001b[38;5;241m.\u001b[39mreshape((seq_len, bs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim))\n",
      "File \u001b[0;32m~/workspace/LightTorch/archive/hw4_extra/./python/needle/init/init_basic.py:52\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(n, i, device, dtype, requires_grad)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate one-hot encoding Tensor\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m device \u001b[38;5;241m=\u001b[39m ndl\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m device\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndl\u001b[38;5;241m.\u001b[39mTensor(\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     53\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     54\u001b[0m     requires_grad\u001b[38;5;241m=\u001b[39mrequires_grad,\n\u001b[1;32m     55\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/LightTorch/archive/hw4_extra/./python/needle/backend_ndarray/ndarray.py:43\u001b[0m, in \u001b[0;36mBackendDevice.one_hot\u001b[0;34m(self, n, i, dtype)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot\u001b[39m(\u001b[38;5;28mself\u001b[39m, n, i, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDArray(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m[i], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/LightTorch/lib/python3.9/site-packages/numpy/lib/_twodim_base_impl.py:231\u001b[0m, in \u001b[0;36meye\u001b[0;34m(N, M, k, dtype, order, device, like)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     i \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mk) \u001b[38;5;241m*\u001b[39m M\n\u001b[0;32m--> 231\u001b[0m \u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_ml import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cuda()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=64, device=device, dtype=\"float32\")\n",
    "model = LanguageModel(20, len(corpus.dictionary), hidden_size=32, num_layers=12, seq_model='transformer', seq_len=20, device=device)\n",
    "train_ptb(model, train_data, seq_len=20, n_epochs=10, device=device, lr=0.003, optimizer=ndl.optim.Adam)\n",
    "evaluate_ptb(model, train_data, seq_len=20, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
